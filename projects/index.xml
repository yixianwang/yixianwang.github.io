<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Yixian Wang – </title>
    <link>https://yixianwang.github.io/projects/</link>
    <description>Recent content on Yixian Wang</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 23 Oct 2023 21:33:41 -0400</lastBuildDate>
    
	  <atom:link href="https://yixianwang.github.io/projects/index.xml" rel="self" type="application/rss+xml" />
    
    
      
        
      
    
    
    <item>
      <title>Exploring GPU Programming with CUDA/CUDA Shared Memory Solving SVM Algorithm</title>
      <link>https://yixianwang.github.io/projects/cuda_svm/</link>
      <pubDate>Wed, 25 Oct 2023 00:57:16 -0400</pubDate>
      
      <guid>https://yixianwang.github.io/projects/cuda_svm/</guid>
      <description>
        
        
        &lt;p&gt;Implemented base &lt;strong&gt;sequential&lt;/strong&gt; version of SVM algorithm on CPU by C++&lt;/p&gt;
      </description>
    </item>
    
    <item>
      <title>Android App: Business Management &amp; Accountant</title>
      <link>https://yixianwang.github.io/projects/android_business_management/</link>
      <pubDate>Wed, 25 Oct 2023 00:15:21 -0400</pubDate>
      
      <guid>https://yixianwang.github.io/projects/android_business_management/</guid>
      <description>
        
        
        &lt;ul&gt;
&lt;li&gt;Designed the app for my parents’ real daily business&lt;/li&gt;
&lt;li&gt;Features: cloud-hosted database, interactive charts, automatic accountant, and PDF generator&lt;/li&gt;
&lt;li&gt;Backend jobs: firebase for authentication, firestore for data and SQL query&lt;/li&gt;
&lt;li&gt;Architecture &amp;amp; Language: MVVM, Kotlin&lt;/li&gt;
&lt;li&gt;Third-party libraries: MPAndroid, Itextpdf, Android-pdf-viewer&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Built &amp;amp; Settings&lt;span class=&#34;absolute -mt-20&#34; id=&#34;built--settings&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#built--settings&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Hardware: Pixel 6 API 32&lt;/li&gt;
&lt;li&gt;The login email: &lt;a href=&#34;mailto:fake@example.com&#34; &gt;fake@example.com&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;The password: 123456&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;App Description&lt;span class=&#34;absolute -mt-20&#34; id=&#34;app-description&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#app-description&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;It is a business management app designed for my parents&amp;rsquo; real daily business of renting houses. It consists of two essential parts, business management and accounting. The inspiration is only from my parent&amp;rsquo;s business which is handled by their handwriting right now. This app will save them much time creating a new contract or appointment. It can also automatically create an accountant summary and some beautiful graphs simultaneously that give my parents an intuitive view of how the business is running. The firebase is heavily used in this project.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;
    &lt;img src=&#34;../images/android_business_management_1.png&#34; alt=&#34;App&#34; loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;h2&gt;APIs &amp;amp; Android Features&lt;span class=&#34;absolute -mt-20&#34; id=&#34;apis--android-features&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#apis--android-features&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;APIs:
&lt;ul&gt;
&lt;li&gt;Firebase for authentication&lt;/li&gt;
&lt;li&gt;Firestore for data backend and SQL query&lt;/li&gt;
&lt;li&gt;MPAndroid for graph&lt;/li&gt;
&lt;li&gt;PDF Generator&lt;/li&gt;
&lt;li&gt;PDF Viewer&lt;/li&gt;
&lt;li&gt;PDF Sharing&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Android features:
&lt;ul&gt;
&lt;li&gt;RecyclerView and Adapter&lt;/li&gt;
&lt;li&gt;Fragment&lt;/li&gt;
&lt;li&gt;Intent&lt;/li&gt;
&lt;li&gt;Coroutines&lt;/li&gt;
&lt;li&gt;CardView&lt;/li&gt;
&lt;li&gt;TableView&lt;/li&gt;
&lt;li&gt;Action Bar&lt;/li&gt;
&lt;li&gt;Date Picker&lt;/li&gt;
&lt;li&gt;Bottom Navigation&lt;/li&gt;
&lt;li&gt;LiveData&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Third-party Libraries &amp;amp; Services Description&lt;span class=&#34;absolute -mt-20&#34; id=&#34;third-party-libraries--services-description&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#third-party-libraries--services-description&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;MPAndroidChart: I use this library to create beautiful interactive pie charts, line charts, and bar charts. The good thing is that I can choose many different charts and customize them. The challenging part is customizing the x-axis because, from its last GitHub update change, the x-axis data only support float data type. That means I always have to use boilerplate code to customize and cast type in the helper function when using the customized x-axis.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Itextpdf: I use this library to customize pdf from the data drawn from firebase. The good thing is that the workflow with it is very straightforward. The challenging part is when I want to add more details to it. The serialized workflow of this library is challenging for adding multiple features or images to the pdf because the previous work can be affected by the latter added features, which might extend pdf page boundaries.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Android-pdf-viewer: I can use this to view pdf smoothly. It provides an efficient pdf viewer service but does not support viewing pdf with other apps.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Firebase: I use this to utilize firebase to store the data and keep them saved and stored in the cloud.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;UI/UX/Display&lt;span class=&#34;absolute -mt-20&#34; id=&#34;uiuxdisplay&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#uiuxdisplay&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;For the “Contract” and “Appointment” parts, I use RecyclerView with CardView for each row. There is an insert button on both of them to insert new data into “Contract” or “Appointment.”&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For the “Home,” there is a manage button on the top left side. We can use it to manage the housing inventory. I also want to mention that only houses in the inventory can be inserted into our “Contract” or “Appointment” list when inserting a new item.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For &amp;ldquo;Accountant,&amp;rdquo; this part is automatically updated with the data in firebase. In addition, TableView has a summary of the current month&amp;rsquo;s and last month&amp;rsquo;s income. It is another essential feature to reduce my parents&amp;rsquo; workload when any of the homeowners want to cash out anytime. I also added three pdf-related features here: creating, viewing, and sharing pdf.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For “Data,” this part is also automatically updated. Here consists of three valuable graphs to give an intuitive and straightforward overlook of how my parents’ business is running.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Backend Processing Logic&lt;span class=&#34;absolute -mt-20&#34; id=&#34;backend-processing-logic&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#backend-processing-logic&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;I have three tables in the firebase database: “allAppointments,” “allContracts,” and “allHouses.”&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The first table, “allAppointments” stores all appointments, including past and upcoming appointments.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The second table, “allContracts” stores all contracts, including old and current underlying contracts.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The third table, “allHouses” handles the housing inventory.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;I have described how these three tables work with UI in section 6 of this report.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Firebase Database Schemas&lt;span class=&#34;absolute -mt-20&#34; id=&#34;firebase-database-schemas&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#firebase-database-schemas&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;p&gt;
    &lt;img src=&#34;../images/android_business_management_2.png&#34; alt=&#34;Firebase 1&#34; loading=&#34;lazy&#34; /&gt;

    &lt;img src=&#34;../images/android_business_management_3.png&#34; alt=&#34;Firebase 2&#34; loading=&#34;lazy&#34; /&gt;

    &lt;img src=&#34;../images/android_business_management_4.png&#34; alt=&#34;Firebase 3&#34; loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
      </description>
    </item>
    
    <item>
      <title>Exploring Parallel Processes Programming with MPICH Simulating Barnes Hut Algorithm</title>
      <link>https://yixianwang.github.io/projects/mpi_barnes_hut/</link>
      <pubDate>Fri, 24 Dec 2021 23:24:06 -0400</pubDate>
      
      <guid>https://yixianwang.github.io/projects/mpi_barnes_hut/</guid>
      <description>
        
        
        &lt;ul&gt;
&lt;li&gt;Implemented astrophysical simulation solved N-body problem using Barnes-Hut algorithm with &lt;strong&gt;MPICH&lt;/strong&gt; by C++&lt;/li&gt;
&lt;li&gt;Programmed &lt;strong&gt;OpenGL&lt;/strong&gt; to visualize the movement of the bodies in the domain by C++&lt;/li&gt;
&lt;li&gt;Analyzed performance with the number of bodies, processors, timesteps, iterations&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Hardware Details &amp;amp; OS Version&lt;span class=&#34;absolute -mt-20&#34; id=&#34;hardware-details--os-version&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#hardware-details--os-version&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Processor:&lt;/strong&gt; 3.6GHz 10-core Intel Core i9&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Memory:&lt;/strong&gt; 32GB 2667 MHz DDR4&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;OS version:&lt;/strong&gt; macOS Big Sur Version 11.6.1&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Data&lt;span class=&#34;absolute -mt-20&#34; id=&#34;data&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#data&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://yixianwang.github.io/datasets/mpi_barnes_hut/nb-10.txt&#34;&gt;Data1: nb-10.txt&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://yixianwang.github.io/datasets/mpi_barnes_hut/nb-100.txt&#34;&gt;Data1: nb-100.txt&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://yixianwang.github.io/datasets/mpi_barnes_hut/nb-100000.txt&#34;&gt;Data1: nb-100000.txt&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Analyze&lt;span class=&#34;absolute -mt-20&#34; id=&#34;analyze&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#analyze&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;My approach can be represented by two main parts: the first is constructing a sequential version of the Barnes-Hut Algorithm and implementing OpenGL to visualize the result, the second is implementing API of MPI to take advantage of parallel from different processes.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In my second part, I tried to implement the method introduced by Grama in the reference paper which is “Scalable Parallel Formulations of the Barnes-Hut Method for n-Body Simulations”, however, in my implementation, after each process computes the trees locally, I cannot get the ideal performance on merging trees part. The paper said the only information that needs to be communicated for merging trees is the number of particles and the center of mass. From my understanding I need to construct a parent node above previous processes, however, I found out that I need to construct the whole new tree in the parent process and other processes need to send the nodes messages of the local tree to parent process because different processes do not share memories and after “Allgather” method, then other processes can traverse the tree during force computing part. These messages passing in tree merging impact performance dramatically in my application and thus in my final submission I didn’t utilize the tree merging method introduced by Grama in the paper.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;However, I implement force computation with MPI and get an obvious performance boost when running my program in multiple processes scenarios. I split input nodes which used to construct the tree into #(number of inputs/number of processes) parts. I set the last rank to be the root process that implements “Gather” and “Scatter” method because, in this setting, it can handle the situation that processes cannot split input nodes equally. And the root process which implements the “Gather” and “Scatter” method does not need to pay attention to the size of messages, because the unevenly be separated input nodes will within the last rank which is the root process.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;
    &lt;img src=&#34;../images/mpi_barnes_hut_1.png&#34; alt=&#34;MPI Barnes Hut Fig 1&#34; loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Fig. 1. Performance measurement by fixing the number of steps(-s parameter to 1000), the number of threshold for MAC(-t parameter to 1.0) and the timestep(-d parameter to 0.04).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;In Fig. 1., I measured the performance of my program from different input sizes by fixing other parameters. In a small input size case(“nb-10.txt”), my program cannot take advantage of the parallel and it can get the best performance when the number of processes is one that is sequentially implemented as shown in the first graph in Fig 1. The main reason is that in a modern computer, the CPU is very powerful it can handle instructions very fast in a sequential style in small input size case. In addition, the small input size will have more overhead on message passing if the number of processes is greater than one which is parallelly implemented. Thus when I increase the number of processes, it will have more overhead on message passing and it will impact the performance of the program. In the (“nb-100.txt”) input scenario, our program can get benefit from parallel. It can get the best performance when the number of processes is around 10 shown as in the second graph in Fig 1, the main reason is that my processor has 10 cores and thus different cores in the processor can handle different processes separately. When I increase the number of processes from 1 to 10, the performance is improved significantly, this indicates that we can get benefit from the computing force in a parallel style. However, when the number of processes is greater than 10, the performance is decreased, some processes are idle during the run time because all 10 cores in processor work on 10 processes separately and the scheduler will let other processes wait for these 10 processes. In a large input size case(“nb-100000.txt”) case, my program has similar improvement when increasing the number of processes. It also can get benefit from the parallel and it can get best performance when the number of processes is around five. However, the performance decreased earlier than the (“nb-100.txt”) case, the main reason is that a large input size has more overhead on the communication between processes during computing force. It has to gather all data computed in local processes in each iteration step and thus the running time in large input size case increasing earlier than previous case.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;
    &lt;img src=&#34;../images/mpi_barnes_hut_2.png&#34; alt=&#34;MPI Barnes Hut Fig 2&#34; loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Fig. 2. Average running time by fixing the number of processes(-np parameter to 4), the number of steps(-s parameter to 1000) and the timestep(-d parameter to 0.04). NAÏVE is the quadratic relationship.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;The running time results in Fig 2 indicates that the Barnes-Hut approximation can significantly speed up computation about 2.5X when theta is setting to 1.0 or 1.5 comparing to the NAÏVE case. As expected, the naïve approach exhibits a quadratic relationship, whereas increasing the theta parameter leads to faster calculations. It does not fare better than the naïve approach until processing the largest input file. Until that point, the overhead of quadtree construction and center of mass calculation outstrips any gains in force estimation. For theta=1 and theta=1.5, however, we see a significant improvement in running time, with similar performance for each.&lt;/li&gt;
&lt;/ul&gt;
      </description>
    </item>
    
    <item>
      <title>Exploring Concurrency Programming with Go</title>
      <link>https://yixianwang.github.io/projects/go_tree_comparison/</link>
      <pubDate>Sun, 24 Oct 2021 17:17:01 -0400</pubDate>
      
      <guid>https://yixianwang.github.io/projects/go_tree_comparison/</guid>
      <description>
        
        
        &lt;ul&gt;
&lt;li&gt;Implemented concurrency programming model to compute BST(binary search tree) equivalence with Go&lt;/li&gt;
&lt;li&gt;Implemented &lt;strong&gt;channels, go-routines, and signaling&lt;/strong&gt; with Go&lt;/li&gt;
&lt;li&gt;Programmed &lt;strong&gt;threads&lt;/strong&gt; to parallelize hash operations with Go&lt;/li&gt;
&lt;li&gt;Assembled a &lt;strong&gt;concurrent buffer&lt;/strong&gt; to secure communication among threads&lt;/li&gt;
&lt;li&gt;Analyzed performance among all the implementations&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Hardware Details &amp;amp; OS Version&lt;span class=&#34;absolute -mt-20&#34; id=&#34;hardware-details--os-version&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#hardware-details--os-version&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;CPU:&lt;/strong&gt; 2 GHz Quad-Core Intel Core i5&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;OS:&lt;/strong&gt; macOS Big Sur version 11.6&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Data&lt;span class=&#34;absolute -mt-20&#34; id=&#34;data&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#data&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://yixianwang.github.io/datasets/go_tree_comparison/simple.txt&#34;&gt;Data1: simple&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://yixianwang.github.io/datasets/go_tree_comparison/coarse.txt&#34;&gt;Data2: coarse&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://yixianwang.github.io/datasets/go_tree_comparison/fine.txt&#34;&gt;Data3: fine&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Analyze HashTime&lt;span class=&#34;absolute -mt-20&#34; id=&#34;analyze-hashtime&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#analyze-hashtime&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;img src=&#34;../images/go_tree_1.png&#34; alt=&#34;image&#34; width=&#34;50%&#34; height=&#34;auto&#34; style=&#34;float: left&#34;&gt;
&lt;img src=&#34;../images/go_tree_2.png&#34; alt=&#34;image&#34; width=&#34;50%&#34; height=&#34;auto&#34; style=&#34;float: right&#34;&gt;
&lt;img src=&#34;../images/go_tree_3.png&#34; alt=&#34;image&#34; width=&#34;50%&#34; height=&#34;auto&#34; style=&#34;float: center&#34;&gt;
&lt;!-- ![HashTime of Simple](images/go_tree_1.png)
![HashTime of Coarse](images/go_tree_2.png)
![HashTime of Fine](images/go_tree_3.png) --&gt;
&lt;blockquote&gt;
&lt;p&gt;Fig. 1. Hash value computing time compared to Goroutines. Different input files has different hash-workers experiments. Computing time are all measured in seconds.  The x-axis represents the number of hash-workers.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The above graph shows the computing time for each BST hashes. The red circle indicates the number of hash-workers equal to the input size for a specific input file, for example, “simple.txt” has an input size of 12. When the input size is small, the sequential version is the fastest. The main reason is that our CPU can solve the problem very fast in sequential when the input size is small and the overhead of the data communication in parallel also influence the performance, and thus in small input size case, the sequential version is the fastest. In a small input size case, the sequential version is about 5X times faster than my second implementation that iterates over the available BSTs. However, when the input size increases and the input dimension increases, we can see from the above graph that the program can get benefit from Goroutine implementations compared to the sequential version.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;My second implementation has better performance than my first implementation in “coarse.txt” and “fine.txt” cases. The main reason is that my laptop has 4 core CPUs, when the number of goroutines reaches the number of cores of my laptop, it can map different jobs to different CPU processors. As the second graph shows, the benefits of parallelism without high amounts of overhead have significant improvement when the number of hash-workers is 4. The third graph above has the same trend when the number of hash-worker is 4. My second implementation performs about 4X times better than my first implementation in both these cases.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Go can manage goroutines very well. The main reason is that goroutines have growable segmented stacks and they grows as needed, and the Go runtime does the scheduling, not the OS.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;However, I still think we have to worry about or pay attention to the number of goroutines because when we know the hardware, we can get the best performance for our program.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If the number of goroutines is less than the number of CPU cores, the program cannot get the best performance on specific hardware even if the Go runtime can schedule the goroutine very well. If the number of goroutines is much more than the number of CPU cores, it will also have some overhead when creating the segment stack even if only 3 registers need when goroutine switch context, and thus I think we also can’t get ideal performance on this setup. In both “coarse.txt” and “fine.txt” cases, I can get the best performance when my second implementation has 4 hash-workers which is the number of the CPU cores and it can also get ideal performance when the number of hash-workers is in a reasonable range. So I think this is kind of proof.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Analyze Averaged HashGroupTime&lt;span class=&#34;absolute -mt-20&#34; id=&#34;analyze-averaged-hashgrouptime&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#analyze-averaged-hashgrouptime&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;img src=&#34;../images/go_tree_4.png&#34; alt=&#34;image&#34; width=&#34;50%&#34; height=&#34;auto&#34; style=&#34;float: left&#34;&gt;
&lt;img src=&#34;../images/go_tree_5.png&#34; alt=&#34;image&#34; width=&#34;50%&#34; height=&#34;auto&#34; style=&#34;float: right&#34;&gt;
&lt;img src=&#34;../images/go_tree_6.png&#34; alt=&#34;image&#34; width=&#34;50%&#34; height=&#34;auto&#34; style=&#34;float: center&#34;&gt;
&lt;!-- ![Averaged HashGroupTime of Simple](images/go_tree_4.png)
![Averaged HashGroupTime of Coarse](images/go_tree_5.png)
![Averaged HashGroupTime of Fine](images/go_tree_6.png) --&gt;
&lt;blockquote&gt;
&lt;p&gt;Fig. 2. Averaged hashGroupTime for different input file. There are four different implementations tested on each input file: Sequential version, Channel version, Lock version and Extra Implementation for extra credit. All computing time are measured in seconds.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;In the “simple.txt” scenario, the sequential version implementation is the fastest. In both the “coarse.txt” and “fine.txt” scenarios, the Channel version implementation is the fastest. The main reason is that when the input size is small, the concurrency mechanism has the drawback that it has to communicate between other goroutines when modifying shared data and in this scenario, this has a significant influence. However, when the input size is large like in the “coarse.txt” case which almost doesn’t have a high amount of overhead, we can see both the Channel version and Lock version can have better performance than the Sequential version and Channel version is the fastest.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In addition, when it comes to the “fine.txt” case, the program also has better performance on the Channel version and the Lock version implementation than the Sequential version, this indicates that our program can scale great in both cases and Channel also is the fastest.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The third implementation has more overhead because other threads must wait for the lock to unlock, and in this trying to unlock process, increases the complexity and overhead. We have to handle the lock and unlock procedure by ourselves.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In the “coarse.txt” case, the Channel version is 2.5X faster than a single thread, and the Lock version is 2.4X faster than a single thread. In addition, the channel implementation is much simpler, the main reason is that I don’t have to worry about the lock, the channel is threaded safe and only one channel changes the data at a time. Furthermore, this result in the channel version has a much simpler implementation, because the channel implicitly handles the lock mechanism, and thus the channel in Go is threaded safe. We don’t have to bother with lock explicitly and this decreases lots of complexity.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For extra credits, I also implement fine-grain synchronization to allow up to data-workers threads to access the data structure at once. It was not access to the shared data structure a bottleneck before. The main reason is that even if it has more parallelism it also has more overhead and it also has to wait for other threads to unlock the shared data structure. These extra implementations also are not simpler than the previous three implementations, because we have to handle the different threads to access data and avoid concurrency errors.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Analyze Averaged CompareTreeTime&lt;span class=&#34;absolute -mt-20&#34; id=&#34;analyze-averaged-comparetreetime&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#analyze-averaged-comparetreetime&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;img src=&#34;../images/go_tree_7.png&#34; alt=&#34;image&#34; width=&#34;50%&#34; height=&#34;auto&#34; style=&#34;float: left&#34;&gt;
&lt;img src=&#34;../images/go_tree_8.png&#34; alt=&#34;image&#34; width=&#34;50%&#34; height=&#34;auto&#34; style=&#34;float: right&#34;&gt;
&lt;img src=&#34;../images/go_tree_9.png&#34; alt=&#34;image&#34; width=&#34;50%&#34; height=&#34;auto&#34; style=&#34;float: center&#34;&gt;
&lt;!-- ![Averaged CompareTreeTime of Simple](images/go_tree_7.png)
![Averaged CompareTreeTime of Coarse](images/go_tree_8.png)
![Averaged CompareTreeTime of Fine](images/go_tree_9.png) --&gt;
&lt;blockquote&gt;
&lt;p&gt;Fig. 3. Averaged compareTreeTime for different input file. There are three different implementation for each input file: Sequential, Goroutine per BST and Concurrent Buffer. All computing time measured in seconds.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The above graph shows the compareTreeTime for each input file on different implementations. When comparing the performance of different implementations, the Go per BST has better performance than the Concurrent Buffer, and the Go per BST implementation also has less complexity than the Concurrent Buffer implementation. The main reason is that when we explore concurrent buffer implementation we have to contain mutex to prevent concurrency errors and it also has additional conditional variables to deal with the buffer size. And thus the Go per BST has better performance and less complexity and less overhead.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Even if the Concurrent buffer is slower than the Go per BST, it still has better performance than a single thread. The Concurrent buffer is 2X faster than single thread implementation and the Go per BST is 2.3X faster than single thread implementation in the “coarse.txt” case. In the “fine.txt” case, the Concurrent buffer is 1.3X faster than single thread implementation and the Go per BST has 2.8X faster than single thread implementation. It is not worth managing the thread pool, because the go can handle the goroutines correctly most of the time if we give a reasonable number of goroutines in our program. In addition, managing the thread pool also increases the complexity and overhead of our program.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;At last, I spent about 11 days in this lab. I don’t have Golang experience before, and thus I spent 3 days on Golang syntax and concurrent knowledge and the other 8 days to work out the result and the report. Although I spent tons of time on this lab, I like the experience in this lab, because I learned a lot from it, like how to debugging and how to manage goroutines and channel correctly, etc. Thanks!&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
      </description>
    </item>
    
    <item>
      <title>Exploring GPU Programming with CUDA/CUDA Shared Memory/Thrust Solving K-Means Algorithm</title>
      <link>https://yixianwang.github.io/projects/cuda_kmeans/</link>
      <pubDate>Sun, 24 Oct 2021 15:21:05 -0400</pubDate>
      
      <guid>https://yixianwang.github.io/projects/cuda_kmeans/</guid>
      <description>
        
        
        &lt;ul&gt;
&lt;li&gt;Implemented base &lt;strong&gt;sequential&lt;/strong&gt; version of K-Means algorithm on CPU by C++&lt;/li&gt;
&lt;li&gt;Implemented first parallel version of K-Means with &lt;strong&gt;Thrust&lt;/strong&gt; primitives and a GroupBy-Aggregate by C++&lt;/li&gt;
&lt;li&gt;Implemented second parallel version of K-Means with &lt;strong&gt;CUDA&lt;/strong&gt; by C++&lt;/li&gt;
&lt;li&gt;Implemented third parallel version of K-Means with &lt;strong&gt;CUDA on Shared Memory&lt;/strong&gt; by C++&lt;/li&gt;
&lt;li&gt;Analyzed speedup among all the implementations&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Hardware Details &amp;amp; OS Version &amp;amp; Settings&lt;span class=&#34;absolute -mt-20&#34; id=&#34;hardware-details--os-version--settings&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#hardware-details--os-version--settings&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;GPU:&lt;/strong&gt; the Codio environment(Tesla T4, Driver Version: 460.91.03 , CUDA Version: 11.2)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CPU:&lt;/strong&gt; the Codio environment(Intel(R) Xeon(R) Platinum 8259CL CPU @ 2.50GHz, 4 cores)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;OS:&lt;/strong&gt; the Codio environment(Ubuntu 18.04.2 LTS (GNU/Linux 5.4.0-1054-aws x86_64))&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;the data in this report was collected when I set “threshold argument” to 1e-6.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Data&lt;span class=&#34;absolute -mt-20&#34; id=&#34;data&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#data&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://yixianwang.github.io/datasets/cuda_kmeans/random-n2048-d16-c16.txt&#34;&gt;Data1: size = 2048, dim = 16, c = 16&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://yixianwang.github.io/datasets/cuda_kmeans/random-n16384-d24-c16.txt&#34;&gt;Data2: size = 16384, dim = 24, c = 16&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://yixianwang.github.io/datasets/cuda_kmeans/random-n65536-d32-c16.txt&#34;&gt;Data3: size = 65536, dim = 32, c = 16&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;The Fastest Implementation&lt;span class=&#34;absolute -mt-20&#34; id=&#34;the-fastest-implementation&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#the-fastest-implementation&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;For averaged elapsed time per iteration, &lt;strong&gt;CUDA Shared Memory is always the fastest&lt;/strong&gt; implementation on different input sizes. (Fig. 1, left) Because the shared memory in each block is very fast, which means the processing unit for each thread can access data in shared memory very fast after transferring data from global memory to the shared memory.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For total elapsed time (E2E runtime), &lt;strong&gt;sequential version is the fastest&lt;/strong&gt; when the input size is very small and &lt;strong&gt;CUDA Shared Memory is the fastest&lt;/strong&gt; when the input size is very large. (Fig. 1, right) The main reason is that the CPU is very powerful in computing a single thread and the CPU doesn’t need to transfer data between host and device. When the input size is very small, it is the fastest. However, when the input size is large, the power and advantage of parallel are obvious, and the overhead over transferring data can be ignored, and in this case, the CUDA Shared Memory is my fastest implementation.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Compare and Analyze Performance Among Four Implementations&lt;span class=&#34;absolute -mt-20&#34; id=&#34;compare-and-analyze-performance-among-four-implementations&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#compare-and-analyze-performance-among-four-implementations&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The &lt;strong&gt;CUDA Shared Memory&lt;/strong&gt; has the smallest elapsed time per iteration (Fig. 1, left) but it also has the largest overhead on transferring data between host and device and between the device and shared memory. It is the fastest implementation when the input size is very large. As the amount of input data increases, it can get more benefit from parallelism and the influence of the overhead can be ignored. (Fig. 1, right)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The &lt;strong&gt;CUDA Basic&lt;/strong&gt; is slightly slower than CUDA Shared Memory on converging speed (Fig. 1, left) and it also has a slightly smaller overhead on transferring data than CUDA Shared Memory, because it doesn’t have to transfer data between the device and shared memory. (Fig. 1, right)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The &lt;strong&gt;Thrust&lt;/strong&gt; is always converging slower than CUDA Basic and has a similar overhead as CUDA Basic. However, it can always converge faster than Sequential implementation. (Fig. 1, left)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The &lt;strong&gt;sequential&lt;/strong&gt; has the slowest converge speed but it doesn’t have any overhead on transferring data. The CPU is very powerful the Sequential version is the fastest implementation for the total elapsed time when the input size is small. (Fig. 1, right)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;
    &lt;img src=&#34;../images/cuda_kmeans_1.png&#34; alt=&#34;cuda kmeans 1&#34; loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;!-- &lt;img src=&#34;../images/cuda_kmeans_1.png&#34; alt=&#34;image&#34; width=&#34;100%&#34; height=&#34;auto&#34;&gt; --&gt;
&lt;blockquote&gt;
&lt;p&gt;Fig. 1. Averaged elapsed time per iteration and Total elapsed time(E2E runtime) measured on different input size.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Estimate the best-case performance of CUDA implementation should have based on the hardware&lt;span class=&#34;absolute -mt-20&#34; id=&#34;estimate-the-best-case-performance-of-cuda-implementation-should-have-based-on-the-hardware&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#estimate-the-best-case-performance-of-cuda-implementation-should-have-based-on-the-hardware&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;Based on the number of threads in my program and the number of processing contexts actually supported by my hardware&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Because a higher occupancy reduces processor idle time(SM may stall due to unavailability of data or busy functional units) and improves overall performance. The best case of performance of CUDA implementation should have the highest occupancy in theory.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;At first, I should list my device info in detail and then use this info to deduce the theoretical best performance of CUDA implementation speedup compared to sequential implementation. I use Codio environment for my lab2. In the Codio environment, the device is Tesla T4, the warp size is 32, the maximum number of threads per block is 1024, the maximum number of blocks is 2147483647. I also found out the bandwidth between Host and Device is 6.3GB/s and the bandwidth between Device to Device is 239.4 GB/s.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Secondly, I ignore the time cost on data transfer and only focus on elapsed time per iteration when estimating the best-case performance, because, from the device info above, we can see that the bandwidth are too big to measure its performance and the “cudaMemCpy” instruction is not an asynchronous instruction(it will cause different performance each time when I test it because it has “barrier” within this kind of function). In addition, the fraction of the end-to-end runtime in CUDA versions is spent in data transfer are the most time-consuming part(Fig. 3.) and the fraction also decreasing when the input size increasing, this is irrelevant with the number of thread or other settings in the program. For these three reasons, it’s fair to ignore the data transfer part when estimating the best performance case. I also assume that the CPU clock cycle per processor is the same as the GPU.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In addition, in my program, I set my number of threads per block in my CUDA implementation to 32, because the warp size of Tesla T4 is 32, and thus the warp schedulers can map each thread to their own position without more or less. Another reason I set the block size to 32 is that after I experimented with the different thread numbers from 16 to 1024 for each given input size and I always got the smallest elapsed time per iteration when the number of threads per block is 32. (I also got the best performance when thread number is 86, the only reason I can come up with is the randomness in bank conflict, because I use double data type throughout in my program, this will produce 2-way bank conflict in 32 warp size situation, every second bank is being asked for 2 different values. The warp scheduler will handle the 2-way bank conflict in random in a sequential style.)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;At last, from all above assumption, the theoretical speedup of my CUDA implementations should have when I set my number of threads in each block to 32, and the hardware in Codio support this setup when the input size smaller than 2147483647 * 32, which is maximum block size times warp size of Tesla T4. In this setup, the warp scheduler won’t let any memory wait for others and thus it has the maximum occupancy.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;
    &lt;img src=&#34;../images/cuda_kmeans_2.png&#34; alt=&#34;cuda kmeans 2&#34; loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Fig. 2. Cuda Shmem, Cuda basic, and Thrust implementations speed up X times compared to sequential kmeans. “2k, 16k and 65k” means “2048, 16384 and 65536” input size. Red color is Cuda Shmem, orange color is Cuda basic and yellow color is Thrust implementation.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Analyze Why Thrust Is Compromising&lt;span class=&#34;absolute -mt-20&#34; id=&#34;analyze-why-thrust-is-compromising&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#analyze-why-thrust-is-compromising&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The Thrust is the slowest parallel implementation, and it does match my expectations. (Fig. 1, left and Fig. 2.)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The first reason is that the advantages of Thrust are abstraction and portability, but under the hood, it has more temporary memory allocations required by the Thrust algorithm during computing than CUDA implementations. For example, in lab2, the Thrust algorithm has additional temporary memory to store the data structure to perform the “reduce_by_key” function. However, we can visually manage every memory in CUDA implementation. Thus, these additional temporary memory allocations can impact Thrust performance.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The second reason is that Thrust cannot make use of shared memory or constant memory in GPU, both two kinds of memories have almost register speed. The Thrust only can make use of global GPU memory and transfer data to computing units through L1 memory to register.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;
    &lt;img src=&#34;../images/cuda_kmeans_3.png&#34; alt=&#34;cuda kmeans 3&#34; loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Fig. 3. The fraction of the end-to-end runtime in CUDA versions(Shared Memory and Basic) is spent in data transfer.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Analyze the Fraction of the End-to-End Runtime of CUDA Implementations Spent in Data Transfer&lt;span class=&#34;absolute -mt-20&#34; id=&#34;analyze-the-fraction-of-the-end-to-end-runtime-of-cuda-implementations-spent-in-data-transfer&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#analyze-the-fraction-of-the-end-to-end-runtime-of-cuda-implementations-spent-in-data-transfer&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;For both CUDA versions (Shared Memory and Basic), the fraction is decreased with the increasing input data size. This indicates that the impact of the overhead of transferring data is decreasing with the increase of input size. (Fig. 3.)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For CUDA Shared Memory has slightly more overhead on transferring data than CUDA basic, because to use shared memory in GPU, it must transfer data from device to shared memory whereas the CUDA Basic only has overhead on transferring data between host and device, it doesn’t need shared memory.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
      </description>
    </item>
    
    <item>
      <title>Exploring Multithreaded Programming with Pthreads Solving Parallel Prefix Scan Algorithm</title>
      <link>https://yixianwang.github.io/projects/pthreads_prefix_sum/</link>
      <pubDate>Fri, 24 Sep 2021 18:44:08 -0400</pubDate>
      
      <guid>https://yixianwang.github.io/projects/pthreads_prefix_sum/</guid>
      <description>
        
        
        &lt;ul&gt;
&lt;li&gt;Implemented base &lt;strong&gt;sequential&lt;/strong&gt; version of work-efficient parallel prefix sum algorithm by C++&lt;/li&gt;
&lt;li&gt;Implemented &lt;strong&gt;parallel&lt;/strong&gt; and &lt;strong&gt;barrier&lt;/strong&gt; versions of work-efficient parallel prefix sum with &lt;strong&gt;POSIX thread (pthread)&lt;/strong&gt; by C++&lt;/li&gt;
&lt;li&gt;Analyzed speedup among the all implantations with respect to the number of threads and data size&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Data&lt;span class=&#34;absolute -mt-20&#34; id=&#34;data&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#data&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://yixianwang.github.io/datasets/pthread_prefix_sum/1k.txt&#34;&gt;Data1: 1k&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://yixianwang.github.io/datasets/pthread_prefix_sum/8k.txt&#34;&gt;Data2: 8k&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://yixianwang.github.io/datasets/pthread_prefix_sum/16k.txt&#34;&gt;Data3: 16k&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://yixianwang.github.io/datasets/pthread_prefix_sum/seq_64_test.txt&#34;&gt;Data4: seq_64_test&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Abstract&lt;span class=&#34;absolute -mt-20&#34; id=&#34;abstract&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#abstract&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;I use Work Efficient Algorithm with building blocks style to compute prefix sum of large array. My work-efficient algorithm has O(log N) time and O(N) work.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Implementation Details for Work-Efficiency Algorithm&lt;span class=&#34;absolute -mt-20&#34; id=&#34;implementation-details-for-work-efficiency-algorithm&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#implementation-details-for-work-efficiency-algorithm&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;I separate input data into same size blocks. (num_blocks = ceil(log2(N)))&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;I compute local prefix sums for these blocks in parallel. It has O(num_blocks) = O(log N) time, and O(num_blocks) * ceil(N / num_blocks) ~ O(N) work.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;I store the last element of each blocks into an 2D array, and then I compute prefix sum of this 2D array in parallel, because this 2D array has very small size than input size, so I choose Hillis’s prefix sum algorithm to compute this 2D array in parallel. The main reason is in this step I do care about the time efficiency not the work efficiency in parallel, and this 2D array has very small size which is equal to the number of blocks, so here we should use work-inefficient algorithm which is Hillis’s prefix sum algorithm. It has O(log(N / num_blocks)) = O(log N) time, and O(N/num_blocks * log(N/num_blocks)) = O(n) work.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;At last, add 2D array to each blocks in proper places in parallel. It also has O(num_blocks) = O(log N) time here, and also has O(num_blocks) * ceil(N / num_blocks) ~ O(N) work.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Analysis:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Time: O(log N)&lt;/li&gt;
&lt;li&gt;Work: O(N)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Analyze Each Step&lt;span class=&#34;absolute -mt-20&#34; id=&#34;analyze-each-step&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#analyze-each-step&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h2&gt;&lt;h3&gt;Step 1&lt;span class=&#34;absolute -mt-20&#34; id=&#34;step-1&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#step-1&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;
    &lt;img src=&#34;../images/pthread_prefixsum_1.png&#34; alt=&#34;Fig 1&#34; loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The above graph can show my program performance when setting LOOP to 100000 on different thread numbers. From the graph, we can see there are two inflection points on each subgraph. The first inflection point is on 2 threads because my work-efficient algorithm after averaging on each thread has additional parallel computing operations on my part 2 implementation (see page 1) than the sequential algorithm. The second inflection point is on 4 threads because my laptop CPU has 4 cores, and thus my program can perform best when setting to 4 threads and each core doesn’t need to switch to another thread, and this can save overhead on context switching.
And with more than 4 threads, the performance of my program decreased, I think the main reason is that there is more overhead when switching threads and that might cause the performance doesn’t improve even decrease. If I setting 4 threads, each core of my 4 core CPU can get only one thread without switching to other threads.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Step 2&lt;span class=&#34;absolute -mt-20&#34; id=&#34;step-2&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#step-2&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;
    &lt;img src=&#34;../images/pthread_prefixsum_2.png&#34; alt=&#34;Fig 2&#34; loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Here, I set my LOOP to 10 and all other arguments keep the same as step1. We decrease the time of each addition operation, and thus our sequential algorithm can get the fastest result. The main reason is that our CPU can solve the problem very fast in sequential as the professor mentioned in the lecture. The decreased amount of time on each addition operation also indicates that the amount of time cost on each thread decreased, and thus more threads will result in more context switch frequency, and thus this will have more overhead than the sequential algorithm.
This is the main reason when the addition operation becoming very fast, our parallel algorithm performance can’t beat the sequential algorithm. Another reason is that our parallel algorithm has more addition operation than sequential algorithm after averaging for each thread as I mentioned on page 1 of my implementation details. This also leads to a performance decrease. The above two reasons can explain why the sequential algorithm is the fastest and why the trend is like this.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;
    &lt;img src=&#34;../images/pthread_prefixsum_3.png&#34; alt=&#34;Fig 3&#34; loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The above graph shows that when setting THREAD number argument to 0 and 4 (0 means sequential algorithm, 4 is because my laptop can get the best performance when threads number is 4), after setting the THREAD argument and changing the LOOP argument to test our program performance. We can see there is an inflection point on each subplot in the red circle because increasing the LOOP argument causes the amount of work on each thread to increase and thus the parallel performance can take advantage of the different workers work at the same time and that meet the performance with the sequential algorithm at the red circle on the graph.
We also notice that by increasing the input size from 64 to 8k, our inflection point moves to the left as indicated in the green box on the graph. The reason is that increasing the numbers of input values, increases the addition operation to both algorithms and this has the same effect as increasing the LOOP. Thus with these two main factors, increasing the LOOP and increasing the input size, can explain why inflection points in these subgraphs of the second graph can meet earlier and earlier, and cause the trend like above.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Step 3&lt;span class=&#34;absolute -mt-20&#34; id=&#34;step-3&#34;&gt;&lt;/span&gt;
    &lt;a href=&#34;#step-3&#34; class=&#34;subheading-anchor&#34; aria-label=&#34;Permalink for this section&#34;&gt;&lt;/a&gt;&lt;/h3&gt;&lt;p&gt;
    &lt;img src=&#34;../images/pthread_prefixsum_4.png&#34; alt=&#34;Fig 4&#34; loading=&#34;lazy&#34; /&gt;

    &lt;img src=&#34;../images/pthread_prefixsum_5.png&#34; alt=&#34;Fig 5&#34; loading=&#34;lazy&#34; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Here, I used my own re-entrant barrier to plot the same graph as before in Step 2. My barrier is implemented with conditional variable and mutex lock. Each thread will wait and try to unlock the barrier in the queue, and once all threads get the signal, the program will go further to the next step.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Spinlock is different from the mutex lock. As the professor said, spinlock causes the thread to keep rolling the lock to try to find out whether the stuff is unlocked or opened all the time. This will constantly cost CPU resources, and if the thread is locked for a long time, spinlock will keep trying to get the control and cause a lot more CPU resources waste.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In the scenarios of the small amount of work in each thread, the spinlock can be better, because spinlock will not waste much CPU time on waiting for the other threads to unlock. It might also boost some performance by trying to get control from those short lifetime threads. And thus in the contrast, in the scenario of a large amount of work in each thread, which means the mutex is better, because in this long waiting time scenarios if some thread wants to unlock and it will wait in a queue instead of waste CPU resources, and won’t constantly cost CPU time.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In addition, a multi-core scenario also can get benefit from the spinlock. When the critical section is small and multi-core environment can reduce the context switch time and that can take advantage of the spinlock.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The main drawback of the spinlock is that it will cost a lot amount of CPU time when the critical section was held for a long time in some thread, and the spinlock will keep trying to unlock the thread, which will waste the CPU resources.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The main drawback of the mutex is that if our program only has a small workload to deal with for each thread, then mutex is very inefficient. Because each thread will go sleep allow another thread to run, however, if the thread has a short lifespan, then the context switching will let the mutex become very inefficient.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;From the above two graphs compared to Step 2, we can see that my own barrier can’t get better performance than the Pthread barrier. From the first graph, we can see the inflection point meets later than the traditional Pthread barrier in Step 2. And from the second graph, we can compare to the average cost time of the program, the green box, indicating that our barrier is getting slower than the Pthread barrier but not that much, it only got 1000 ms average worse. But basically, the results are in line with my expectation, because the logic behind the Phtread is similar to mine. The main reason that my barrier is slightly inefficient is that I let my barrier spin a little bit to try to unlock the thread, which would cost some CPU time, and that will cause worse performance.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;I suggest using my own spinlock barrier to some tasks or some scenarios that have some small subtasks that only have a short lifespan, and the rest of the tasks has a large workload. In this scenario, my barrier will work better. Otherwise, if the tasks are all with a large workload, then I suggest using the Pthread barrier instead because it doesn’t waste any time on trying to unlock, it will go to sleep and wait in a queue and let other threads use the resources.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
      </description>
    </item>
    
    <item>
      <title>Self-driving SuperTuxKart with Pytorch</title>
      <link>https://yixianwang.github.io/projects/supertuxkart/</link>
      <pubDate>Fri, 25 Jun 2021 02:08:49 -0400</pubDate>
      
      <guid>https://yixianwang.github.io/projects/supertuxkart/</guid>
      <description>
        
        
        &lt;ul&gt;
&lt;li&gt;Designed deep networks for a racing simulator, SuperTuxKart with Pytorch&lt;/li&gt;
&lt;li&gt;Trained linear model and multi-layer perceptron model to classify images from SuperTuxKart&lt;/li&gt;
&lt;li&gt;Trained a convolutional network to classify images from SuperTuxKart&lt;/li&gt;
&lt;li&gt;Built classification network fully convolutional and solved a semantic labeling task (labeling every pixel
in the image)&lt;/li&gt;
&lt;li&gt;Implemented an object detector&lt;/li&gt;
&lt;li&gt;Trained a CNN to do vision-based self-driving in SuperTuxKart&lt;/li&gt;
&lt;li&gt;Programmed a SuperTuxKart ice-hockey player (AI that plays ice-hockey)&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;[Resources](I also have four years of experience as a Data Scientist before pursuing a master’s degree.)&lt;/li&gt;
&lt;/ul&gt;
      </description>
    </item>
    
    <item>
      <title>Semantic Parsing with Encoder-Decoder Models</title>
      <link>https://yixianwang.github.io/projects/nlp_semantic_parsing_encoder_decoder/</link>
      <pubDate>Sun, 25 Apr 2021 02:18:07 -0400</pubDate>
      
      <guid>https://yixianwang.github.io/projects/nlp_semantic_parsing_encoder_decoder/</guid>
      <description>
        
        
        &lt;ul&gt;
&lt;li&gt;Implemented an &lt;strong&gt;Encoder-Decoder model&lt;/strong&gt; for semantic parsing with Pytorch&lt;/li&gt;
&lt;li&gt;Implemented a decoder by using &lt;strong&gt;LTSM&lt;/strong&gt; whose output is passed to a feedforward layer and a softmax over the vocabulary&lt;/li&gt;
&lt;li&gt;Added &lt;strong&gt;attention&lt;/strong&gt; mechanisms to the model to make it more powerful and faster to train&lt;/li&gt;
&lt;/ul&gt;
      </description>
    </item>
    
  </channel>
</rss>
