{"/notes/colab_tips/":{"data":{"":"colab tips Seems like colab is popular, so I wanted to share maybe some useful tips that helped me streamline my setup/workflow:\na) If you cloned the notebook and are using gdrive to store the data, you can only mount gdrive using the python package, which requires entering a new oauth code for each session; if you create a new notebook, it actually persists the mount between sessions. You can just copy the contents over - https://datascience.stackexchange.com/questions/64486/how-to-automatically-mount-my-google-drive-to-google-colab\nb) You can run this code in a cell to extend sessions (prevent timeouts); it also doesn\u0026rsquo;t majorly use the CPU unnecessarily. Still, as this SO points out, it\u0026rsquo;s not morally right to hog up a GPU/CPU if you\u0026rsquo;re not using it. I use it when I\u0026rsquo;m actively developing over ssh, but not running anything on the notebook.\nimport time while True: time.sleep(10) c) You can ssh into the machine; someone even made a package for that - https://pypi.org/project/colab-ssh/#description. I usually use ipdb to do REPL-driven development and explore the data, so this is really useful for me since I can\u0026rsquo;t run this workload locally. Head\u0026rsquo;s up - it does take a bit of setup, but if you persist the extra python packages and the authorized_hosts file, you can get it up and running for new sessions quickly.\nimport os import sys # set up persistent pip library path nb_path = \u0026#39;/content/drive/My Drive/Colab Notebooks/pip\u0026#39; !mkdir -p \u0026#39;{nb_path}\u0026#39; sys.path.insert(0, nb_path) !pip install --target=\u0026#39;{nb_path}\u0026#39; ipdb !pip install --target=\u0026#39;{nb_path}\u0026#39; colab_ssh --upgrade !ln -sr /content/drive/MyDrive/nlp-qa-finalproj/.ssh ~/ !cat ~/.ssh/authorized_keys Hope this helps, good luck!\n"},"title":"colab_tips"},"/notes/colab_upload/":{"data":{"":"import os from getpass import getpass import urllib\nuser = input(\u0026lsquo;User name: \u0026lsquo;) password = getpass(\u0026lsquo;Password: \u0026lsquo;) password = urllib.parse.quote(password) # your password is converted into url format repo_name = \u0026ldquo;gregdurrett/nlp-qa-finalproj.git\u0026rdquo; cmd_string = \u0026lsquo;git clone https://{0}:{1}@github.com/{2}\u0026rsquo;.format(user, password, repo_name)\n!{cmd_string}\n"},"title":"colab_upload"},"/notes/conda/":{"data":{"":"conda env list conda create -n dl1 python=3.8 \u0026ndash;no-default-packages conda env remove -n ENV_NAME\n"},"title":"conda"},"/notes/cpp/":{"data":{"":"","assert#assert":"","c-const#c++ const":" link // value of x and y can be altered // x = 9; y = \u0026#39;A\u0026#39;; // value of i and j can be altered // i = \u0026amp;m; j = \u0026amp;n; // !!! value of *i and *j cannot be altered // *i = 6; *j = 7; // read-only variable is not assignable const int* i = \u0026amp;x; const char* j = \u0026amp;y; // value of x and y can be altered // x = 9; y = \u0026#39;A\u0026#39;; // !!! value of i and j cannot be altered // i = \u0026amp;m; j = \u0026amp;n; // variable \u0026#39;i\u0026#39; and \u0026#39;j\u0026#39; declared const here // value of *i and *j can be altered // *i = 6; *j = \u0026#39;A\u0026#39;; int* const i = \u0026amp;x; char* const j = \u0026amp;y; // value of x and y can be altered // x = 9; y = \u0026#39;A\u0026#39;; // !!! value of i and j cannot be altered // i = \u0026amp;m; j = \u0026amp;n; // !!! value of *i and *j cannot be altered // *i = 6; *j = 7; const int* const i = \u0026amp;x; const char* const j = \u0026amp;y; The compile-time error that will appear as if const value is passed to any non-const argument of the function\nint foo(int* y) { return *y; } int main() { int z = 8; const int* x = \u0026amp;z; std::cout \u0026lt;\u0026lt; foo(x) \u0026lt;\u0026lt; std::endl; return 0; } // error: no matching function for call to \u0026#39;foo\u0026#39; // candidate function not viable: 1st argument (\u0026#39;const int *\u0026#39;) would lose const qualifier const int foo(int* y) { return *y; } int main() { int z = 8; const int* x = \u0026amp;z; std::cout \u0026lt;\u0026lt; foo(x) \u0026lt;\u0026lt; std::endl; return 0; } // Function foo() with variable // const int void foo(const int y) { // y = 6; const value // can\u0026#39;t be change cout \u0026lt;\u0026lt; y; } // Function foo() with variable int void foo1(int y) { // Non-const value can be change y = 5; cout \u0026lt;\u0026lt; \u0026#39;\\n\u0026#39; \u0026lt;\u0026lt; y; } // Driver Code int main() { int x = 9; const int z = 10; foo(z); foo1(x); return 0; } const return\n// int foo(int y) { // no error // const int foo(int y) { // no error const int foo(const int y) { // error: cannot assign to variable \u0026#39;y\u0026#39; with const-qualified type \u0026#39;const int\u0026#39; --y; return y; } int main() { int x = 9; const int z = 10; std::cout \u0026lt;\u0026lt; foo(x) \u0026lt;\u0026lt; \u0026#39;\\n\u0026#39; \u0026lt;\u0026lt; foo(z); return 0; } An object declared as const cannot be modified and hence, can invoke only const member functions as these functions ensure not to modify the object.\nWhen a function is declared as const, it can be called on any type of object, const object as well as non-const objects.\nclass Test { public: // Constructor Test(int v = 0) { value = v; } // this const means cannot modify class members, e.g. value // We get compiler error if we add a line like \u0026#34;value = 100;\u0026#34; // in this function. int getValue() const { return value; } // a nonconst function trying to modify value void setValue(int val) { value = val; } private: int value; }; // Driver Code int main() { // Object of the class T Test t(20); // non-const object invoking const function, no error cout \u0026lt;\u0026lt; t.getValue() \u0026lt;\u0026lt; endl; // const object const Test t_const(10); // const object invoking const function, no error cout \u0026lt;\u0026lt; t_const.getValue() \u0026lt;\u0026lt; endl; // const object invoking non-const function, CTE // t_const.setValue(15); // non-const object invoking non-const function, no error t.setValue(12); cout \u0026lt;\u0026lt; t.getValue() \u0026lt;\u0026lt; endl; return 0; } ","c20-comparison-operator#C++20 comparison operator":"struct Point { int x; int y; Point() : x(0), y(0) {} Point(int a, int b) : x(a), y(b) {} // !!! have to write it this way: inline bool operator== (const Point\u0026amp; other) const { return x == other.x \u0026amp;\u0026amp; y == other.y; } }; ","customized-hash-for-unordered_map-or-unordered_set#customized hash for unordered_map or unordered_set":"struct pair_hash { template \u0026lt;class T1, class T2\u0026gt; std::size_t operator () (const std::pair\u0026lt;T1,T2\u0026gt; \u0026amp;p) const { auto h1 = std::hash\u0026lt;T1\u0026gt;{}(p.first); auto h2 = std::hash\u0026lt;T2\u0026gt;{}(p.second); // Mainly for demonstration purposes, i.e. works but is overly simple // In the real world, use sth. like boost.hash_combine return h1 ^ (h2 \u0026lt;\u0026lt; 1); } }; int main() { std::unordered_map\u0026lt;std::pair\u0026lt;int, int\u0026gt;, int, pair_hash\u0026gt; pos_index_map; return 0; } ","element-wise-comparison-of-two-structs#element wise comparison of two structs":"struct Point { float x; float y; Point(int x = 0, int y = 0) : x(x), y(y) {} }; int main() { Point p1 = Point(1, 2); Point p2 = Point(2, 1); // std::tie can have any many parameters as it wants if (std::tie(p1.x, p2.x) == std::tie(p2.y, p1.y)) { std::cout \u0026lt;\u0026lt; \u0026#34;haha\u0026#34; \u0026lt;\u0026lt; std::endl; } else { std::cout \u0026lt;\u0026lt; \u0026#34;nono\u0026#34; \u0026lt;\u0026lt; std::endl; } } ","function-pointer-in-c#function pointer in c++":"int sum(int a, int b) { return a + b; } int prod(int a, int b) { return a * b; } int shouldNotBeChanged(int (*operation)(int, int)) { srand(time(nullptr)); int a = rand() % 100; int b = rand() % 100; printf(\u0026#34;The result of the operation between %d and %d is %d\\n\u0026#34;, a, b, operation(a, b)); return 0; } int main() { shouldNotBeChanged(\u0026amp;sum); return 0; } ","gtest-with-cmake#gtest with cmake":"","hashmap-implementation#HashMap Implementation":" leetcode 705 design hashset // C++ ","how-to-use-c-build-in-hash-function#how to use c++ build-in hash function":" size_t h1 = std::hash\u0026lt;char\u0026gt;()(\u0026#39;a\u0026#39;); size_t h2 = std::hash\u0026lt;char\u0026gt;()(\u0026#39;b\u0026#39;); std::unordered_map\u0026lt;std::string, int\u0026gt; myhash; std::unordered_map\u0026lt;std::string, int\u0026gt;::hasher fn = myhash.hash_function(); std::cout \u0026lt;\u0026lt; fn(\u0026#34;apple\u0026#34;) \u0026lt;\u0026lt; std::endl; ","largest-divisible-subsethttpswwwjiuzhangcomproblemlargest-divisible-subset#\u003ca href=\"https://www.jiuzhang.com/problem/largest-divisible-subset/\"\u003eLargest Divisible Subset\u003c/a\u003e":"assert #include\u0026lt;cassert\u0026gt; assert((expression) \u0026amp;\u0026amp; \u0026#34;msg\u0026#34;) assert(expression); // cannot be std::assert(expression) try throw catch - error handling try { // do something that might throw an error throw std::invalid_argument(\u0026#34;MyFunc argument too large.\u0026#34;); } catch (const std::exception\u0026amp; e) { // handle the error std::cout \u0026lt;\u0026lt; \u0026#34;3333\u0026#34; \u0026lt;\u0026lt; n \u0026lt;\u0026lt; std::endl; std::cerr \u0026lt;\u0026lt; e.what() \u0026lt;\u0026lt; std::endl; // return -1; } gtest with cmake gtest helloworld step 1: mkdir my_project \u0026amp;\u0026amp; cd my_project step 2: CMakeLists.txt # within CMakeLists.txt cmake_minimum_required(VERSION 3.14) project(my_project) # GoogleTest requires at least C\u0026#43;\u0026#43;14 set(CMAKE_CXX_STANDARD 14) set(CMAKE_CXX_STANDARD_REQUIRED ON) include(FetchContent) FetchContent_Declare( googletest URL https://github.com/google/googletest/archive/03597a01ee50ed33e9dfd640b249b4be3799d395.zip ) # For Windows: Prevent overriding the parent project\u0026#39;s compiler/linker settings set(gtest_force_shared_crt ON CACHE BOOL \u0026#34;\u0026#34; FORCE) FetchContent_MakeAvailable(googletest) step 3: test fucntions #include \u0026lt;gtest/gtest.h\u0026gt; // Demonstrate some basic assertions. TEST(HelloTest, BasicAssertions) { // Expect two strings not to be equal. EXPECT_STRNE(\u0026#34;hello\u0026#34;, \u0026#34;world\u0026#34;); // Expect equality. EXPECT_EQ(7 * 6, 42); } step 4: Append to CMakeLists.txt enable_testing() add_executable( hello_test hello_test.cc ) target_link_libraries( hello_test GTest::gtest_main ) include(GoogleTest) gtest_discover_tests(hello_test) step 5: build and run test my_project$ cmake -S . -B build -- The C compiler identification is GNU 10.2.1 -- The CXX compiler identification is GNU 10.2.1 ... -- Build files have been written to: .../my_project/build my_project$ cmake --build build Scanning dependencies of target gtest ... [100%] Built target gmock_main my_project$ cd build \u0026amp;\u0026amp; ctest Test project .../my_project/build Start 1: HelloTest.BasicAssertions 1/1 Test #1: HelloTest.BasicAssertions ........ Passed 0.00 sec 100% tests passed, 0 tests failed out of 1 Total Test time (real) = 0.01 sec print vector to the console std::copy(v.begin(), v.end(), std::ostream_iterator\u0026lt;int\u0026gt;(std::cout, \u0026#34; \u0026#34;)); priority queue 1507 Shortest Subarray with Sum at Least K 和至少为 K 的最短子数组 [[https://www.lintcode.com/problem/1507/][Lintcode 1507 Shortest Subarray with Sum at Least K]]\nBinary search on answer + priority_queue class Solution { public: int shortestSubarray(std::vector\u0026lt;int\u0026gt;\u0026amp; A, int K) { std::vector\u0026lt;int\u0026gt; prefix_sum = GetPrefixSum(A); int left = 1; int right = A.size(); while (left + 1 \u0026lt; right) { int mid = left + (right - left) / 2; if (IsValid(prefix_sum, mid, K)) { right = mid; } else { left = mid; } } if (IsValid(prefix_sum, left, K)) { return left; } if (IsValid(prefix_sum, right, K)) { return right; } return -1; } private: std::vector\u0026lt;int\u0026gt; GetPrefixSum(std::vector\u0026lt;int\u0026gt;\u0026amp; nums) { std::vector\u0026lt;int\u0026gt; answer(nums.size() + 1, 0); for (int i = 0; i \u0026lt; nums.size(); ++i) { answer[i + 1] = answer[i] + nums[i]; } return answer; } bool IsValid(std::vector\u0026lt;int\u0026gt;\u0026amp; prefix_sum, int length, int K) { auto cmp = [](const std::pair\u0026lt;int, int\u0026gt;\u0026amp; a, const std::pair\u0026lt;int, int\u0026gt;\u0026amp; b) { return a.second \u0026gt; b.second; }; std::set\u0026lt;std::pair\u0026lt;int, int\u0026gt;, decltype(cmp)\u0026gt; pq(cmp); // c++20 pq; c++11 pq(cmp) for (int end = 0; end \u0026lt; prefix_sum.size(); ++end) { int index = end - length - 1; if (index \u0026gt;= 0) { pq.erase(std::find_if(pq.begin(), pq.end(), [\u0026amp;index](const std::pair\u0026lt;int, int\u0026gt;\u0026amp; a) { return a.first == index; })); } if (!pq.empty() \u0026amp;\u0026amp; prefix_sum[end] - pq.rbegin()-\u0026gt;second \u0026gt;= K) { return true; } pq.insert(std::make_pair(end, prefix_sum[end])); } return false; } }; LRU implementation // C++ #include \u0026lt;unordered_map\u0026gt; struct LinkedNode { LinkedNode(int key, int value, LinkedNode* next) : key(key), value(value), next(next) {} int key; int value; LinkedNode* next; }; class LRUCache { public: LRUCache(int capacity) : capacity_(capacity), dummy_(new LinkedNode(0, 0, nullptr)), tail_(dummy_) {} // Google style: Get int Get(int key) { if (key_to_previous_.find(key) == key_to_previous_.end()) { return -1; } LinkedNode* previous = key_to_previous_.at(key); LinkedNode* current = previous-\u0026gt;next; Kick(previous); return current-\u0026gt;value; } // Google style: Set void Set(int key, int value) { if (key_to_previous_.find(key) != key_to_previous_.end()) { Kick(key_to_previous_.at(key)); tail_-\u0026gt;value = value; return; } PushBack(new LinkedNode(key, value, nullptr)); // 如果key不存在，则存入新节点 if (key_to_previous_.size() \u0026gt; capacity_) { // 如果缓存超出上限 PopFront(); } } private: void PushBack(LinkedNode* node) { key_to_previous_[node-\u0026gt;key] = tail_; tail_-\u0026gt;next = node; tail_ = node; } void PopFront() { // 删除头部 LinkedNode* head = dummy_-\u0026gt;next; key_to_previous_.erase(head-\u0026gt;key); dummy_-\u0026gt;next = head-\u0026gt;next; key_to_previous_[head-\u0026gt;next-\u0026gt;key] = dummy_; } // change \u0026#34;previous-\u0026gt;node-\u0026gt;next-\u0026gt;...-\u0026gt;tail_\u0026#34; // to \u0026#34;previous-\u0026gt;next-\u0026gt;...-\u0026gt;tail_-\u0026gt;node\u0026#34; void Kick(LinkedNode* previous) { // 将数据移至尾部 LinkedNode* node = previous-\u0026gt;next; if (node == tail_) { return; } // update the current node from linked list previous-\u0026gt;next = node-\u0026gt;next; // update the previous node in hash map key_to_previous_[node-\u0026gt;next-\u0026gt;key] = previous; node-\u0026gt;next = nullptr; PushBack(node); } int capacity_; LinkedNode* dummy_; LinkedNode* tail_; std::unordered_map\u0026lt;int, LinkedNode*\u0026gt; key_to_previous_; }; LIS Longest Increasing Subsequence 接龙规则：从左到右一个比一个大，该问题简称 LIS 状态表示： A：dp[i] 表示前i个数的 LIS 是多长(前缀型, do not choose this) B：dp[i] 表示以第i个数结尾的 LIS 是多长(坐标型) LIS 的动态规划四要素 state: dp[i]表示以第i个数为龙尾的最长的龙有多长 function: dp[i] = max{dp[j] + 1}, j \u0026lt; i \u0026amp;\u0026amp; nums[j] \u0026lt; nums[i] initialization: dp[0..n-1] = 1 answer: max{dp[0..n-1]} def longestIncreasingSubsequence(self, nums): if nums is None or not nums: return 0 # state: dp[i] 表示以第i个数结尾的LIS的长度 # initialization：dp[0..n-1] = 1 dp = [1] * len(nums) # function: dp[i] = max(dp[i] + 1), j \u0026lt; i \u0026amp;\u0026amp; nums[j] \u0026lt; nums[i] for i in range(len(nums)): for j in range(i): if nums[j] \u0026lt; nums[i]: dp[i] = max(dp[i], dp[j] + 1) # answer, 任意一个位置都可能是LIS的结尾 return max(dp) 改动要点(返回最优方案) prev 数组记录前继最优状态 max() 的写法要改为 if 的写法 找到最长龙的结尾，从结尾倒推出整条龙 def longestIncreasingSubsequence(self, nums): if nums is None or not nums: return 0 # state: dp[i] 表示以第i个数结尾的LIS的长度 # initialization：dp[0..n-1] = 1 dp = [1] * len(nums) # prev[i]代表dp[i]的最优值是从哪个dp[j]算过来的 prev = [-1] * len(nums) # function dp[i] = max{dp[j] + 1}, j \u0026lt; i and nums[j] \u0026lt; nums[i] for i in range(len(nums)): for j in range(i): if nums[j] \u0026lt; nums[i] and dp[i] \u0026lt; dp[j] + 1: dp[i] = dp[j] + 1 prev[i] = j # answer: max(dp[0..n-1]) longest, last = 0, -1 for i in range(len(nums)): if dp[i] \u0026gt; longest: longest = dp[i] last = i path = [] while last != -1 path.append(nums[last]) last = prev[last] print(path[::-1]) return longest LIS2 Longest Continuous Increasing Subsequence 2 class Solution: \u0026#34;\u0026#34;\u0026#34; @param A: An integer matrix @return: an integer \u0026#34;\u0026#34;\u0026#34; def longestContinuousIncreasingSubsequence2(self, A): if not A or not A[0]: return 0 n, m = len(A), len(A[0]) points = [] for i in range(n): for j in range(m): points.append((A[i][j], i, j)) points.sort() longest_hash = {} for i in range(len(points)): key = (points[i][1], points[i][2]) longest_hash[key] = 1 for dx, dy in [(1, 0), (0, -1), (-1, 0), (0, 1)]: x, y = points[i][1] + dx, points[i][2] + dy if x \u0026lt; 0 or x \u0026gt;= n or y \u0026lt; 0 or y \u0026gt;= m: continue if (x, y) in longest_hash and A[x][y] \u0026lt; points[i][0]: longest_hash[key] = max(longest_hash[key], longest_hash[(x, y)] + 1) return max(longest_hash.values()) Largest Divisible Subset class Solution: def largestDivisibleSubset(self, nums): if not nums: return [] nums = sorted(nums) n = len(nums) dp, prev = {}, {} for num in nums: dp[num] = 1 prev[num] = -1 last_num = nums[0] for num in nums: for factor in self.get_smaller_factors(num): if factor not in dp: continue if dp[num] \u0026lt; dp[factor] + 1: dp[num] = dp[factor] + 1 prev[num] = factor if dp[num] \u0026gt; dp[last_num]: last_num = num return self.get_path(prev, last_num) def get_smaller_factors(self, num): if num == 1: return [] factor = 1 factors = [] while factor * factor \u0026lt;= num: if num % factor == 0: factors.append(factor) if factor * factor != num and factor != 1: factors.append(num // factor) factor += 1 return factors def get_path(self, prev, last_num): path = [] while last_num != -1: path.append(last_num) last_num = prev[last_num] return path[::-1] ","lis-longest-increasing-subsequencehttpswwwjiuzhangcomproblemlongest-increasing-subsequence#\u003ca href=\"https://www.jiuzhang.com/problem/longest-increasing-subsequence/\"\u003eLIS Longest Increasing Subsequence\u003c/a\u003e":"","lis2-longest-continuous-increasing-subsequence-2httpswwwjiuzhangcomproblemlongest-continuous-increasing-subsequence-ii#\u003ca href=\"https://www.jiuzhang.com/problem/longest-continuous-increasing-subsequence-ii/\"\u003eLIS2 Longest Continuous Increasing Subsequence 2\u003c/a\u003e":"","lru-implementationhttpswwwlintcodecomproblem134#\u003ca href=\"https://www.lintcode.com/problem/134/\"\u003eLRU implementation\u003c/a\u003e":"","print-vector-to-the-console#print vector to the console":"","priority-queue#priority queue":"","random-seed#random seed":"3407","return-min-or-max-element-from-hashmap#return min or max element from hashmap":"auto cmp = [](const auto\u0026amp; a, const auto\u0026amp; b) {return a.second \u0026lt; b.second;}; min_value = min_element(my_map.begin(), my_map.end(), cmp)-\u0026gt;second; ","sort-lambda#sort lambda":" auto sortRuleLambda = [](const Skyscraper\u0026amp; s1, const Skyscraper\u0026amp; s2) -\u0026gt; bool { return s1.height() \u0026lt; s2.height(); }; std::sort(skyscrapers.begin(), skyscrapers.end(), sortRuleLambda); ","to-initialize-two-dimentional-array#To initialize two dimentional array":" we cannot iterate \u0026lsquo;priority_queue\u0026rsquo;, but we can make a copy of it and then use \u0026lsquo;pop\u0026rsquo; and \u0026rsquo;top()\u0026rsquo; to iterate #include\u0026lt;iostream\u0026gt; int main() { int** secondStore; secondStore = new int*[10]; for (int i = 0; i \u0026lt; 10; ++i) { secondStore[i] = new int[32]; } std::cout \u0026lt;\u0026lt; secondStore[0][0] \u0026lt;\u0026lt; std::endl; return 0; } ## heap: set vs priority_queue ```c++ #include \u0026lt;iostream\u0026gt; #include \u0026lt;string\u0026gt; #include \u0026lt;vector\u0026gt; #include \u0026lt;set\u0026gt; #include \u0026lt;iterator\u0026gt; #include \u0026lt;math.h\u0026gt; #include \u0026lt;cassert\u0026gt; #include \u0026lt;queue\u0026gt; #include \u0026lt;map\u0026gt; int main() { auto cmp = [](const std::pair\u0026lt;int, int\u0026gt;\u0026amp; a, const std::pair\u0026lt;int, int\u0026gt;\u0026amp; b) {return a.first \u0026gt; b.first;}; std::set\u0026lt;std::pair\u0026lt;int, int\u0026gt;, decltype(cmp)\u0026gt; my_heap_with_set(cmp); // get min heap // std::priority_queue\u0026lt;std::pair\u0026lt;int, int\u0026gt;, std::deque\u0026lt;std::pair\u0026lt;int, int\u0026gt;\u0026gt;, decltype(cmp)\u0026gt; my_heap_with_priority_queue(cmp); // get max heap std::priority_queue\u0026lt;std::pair\u0026lt;int, int\u0026gt;, std::deque\u0026lt;std::pair\u0026lt;int, int\u0026gt;\u0026gt;\u0026gt; my_heap_with_priority_queue; // get max heap my_heap_with_set.insert(std::make_pair(3, 1)); my_heap_with_set.insert(std::make_pair(2, 1)); my_heap_with_set.insert(std::make_pair(4, 1)); my_heap_with_set.insert(std::make_pair(0, 1)); my_heap_with_priority_queue.push({3, 1}); my_heap_with_priority_queue.push({2, 1}); my_heap_with_priority_queue.push({4, 1}); my_heap_with_priority_queue.push({0, 1}); auto it = my_heap_with_set.begin(); std::cout \u0026lt;\u0026lt; \u0026#34;my_set: \u0026#34; \u0026lt;\u0026lt; std::endl;; for (; it != my_heap_with_set.end(); ++it) { std::cout \u0026lt;\u0026lt; it-\u0026gt;first \u0026lt;\u0026lt; \u0026#34; \u0026#34; \u0026lt;\u0026lt; it-\u0026gt;second \u0026lt;\u0026lt; std::endl; } std::cout \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; \u0026#34;my_priority_queue: \u0026#34; \u0026lt;\u0026lt; std::endl; for (; !my_heap_with_priority_queue.empty(); my_heap_with_priority_queue.pop()) { std::cout \u0026lt;\u0026lt; my_heap_with_priority_queue.top().first \u0026lt;\u0026lt; \u0026#34; \u0026#34; \u0026lt;\u0026lt; my_heap_with_priority_queue.top().second \u0026lt;\u0026lt; std::endl; } std::cout \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; \u0026#34;test map iteration: \u0026#34; \u0026lt;\u0026lt; std::endl; auto cmp2 = [](const int\u0026amp; a, const int\u0026amp; b) {return a \u0026gt; b;}; std::map\u0026lt;int, int, decltype(cmp2)\u0026gt; my_map(cmp2); my_map[0] = 12; my_map[1] = 15; my_map[1111] = 111; for (auto i : my_map) { // works std::cout \u0026lt;\u0026lt; i.first \u0026lt;\u0026lt; \u0026#34; \u0026#34; \u0026lt;\u0026lt; i.second \u0026lt;\u0026lt; std::endl; // for (auto it = my_map.begin(); it != my_map.end(); ++it) { // works // std::cout \u0026lt;\u0026lt; it-\u0026gt;first \u0026lt;\u0026lt; \u0026#34; \u0026#34; \u0026lt;\u0026lt; it-\u0026gt;second \u0026lt;\u0026lt; std::endl; } std::cout \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; \u0026#34;test max_element for map: \u0026#34; \u0026lt;\u0026lt; std::endl; auto cmp_max_element = [](const auto\u0026amp; a, const auto\u0026amp; b) {return a.second \u0026gt; b.second;}; // be aware that we should use \u0026#39;-\u0026gt;second\u0026#39; at the end, becuase max_element return iterator int temp = max_element(my_map.begin(), my_map.end(), cmp_max_element)-\u0026gt;second; std::cout \u0026lt;\u0026lt; temp \u0026lt;\u0026lt; std::endl; std::cout \u0026lt;\u0026lt; std::endl; return 0; } ","try-throw-catch---error-handling#try throw catch - error handling":""},"title":"cpp"},"/notes/cuda/":{"data":{"":"CUDA (Compute Unified Device Architecture) is a parallel computing platform and application programming interface (API) created by NVIDIA for utilizing their GPUs (Graphics Processing Units) to accelerate various computational tasks, including machine learning. Many machine learning algorithms can be optimized with CUDA to take advantage of GPU parallelism, which can significantly speed up training and inference. Here are some common machine learning algorithms that can benefit from CUDA optimization:\nDeep Learning Algorithms:\nConvolutional Neural Networks (CNNs): Used in image and video analysis, CNNs can be accelerated with CUDA for image recognition, object detection, and more. Recurrent Neural Networks (RNNs): RNNs, especially in natural language processing tasks, can benefit from GPU acceleration. Support Vector Machines (SVM): SVMs are used for classification and regression tasks. Training large SVMs can be time-consuming, and CUDA can speed up the process.\nk-Nearest Neighbors (k-NN): CUDA can accelerate the distance calculations required in k-NN algorithms.\nRandom Forests: Implementations of random forests can be parallelized on GPUs for faster training.\nGradient Boosting Algorithms: Some gradient boosting libraries, like XGBoost and LightGBM, have GPU support to speed up boosting algorithms\u0026rsquo; training.\nMatrix Factorization: Algorithms like Singular Value Decomposition (SVD) and Alternating Least Squares (ALS) used in recommendation systems can benefit from GPU acceleration.\nClustering Algorithms: Algorithms like K-means clustering and DBSCAN can be optimized with CUDA to speed up clustering tasks on large datasets.\nPrincipal Component Analysis (PCA): PCA, a dimensionality reduction technique, can be accelerated with CUDA when working with high-dimensional data.\nNon-negative Matrix Factorization (NMF): NMF is used in various applications like topic modeling and image processing and can be accelerated using CUDA.\nEnsemble Methods: Bagging and boosting techniques that involve multiple base models can be optimized with CUDA.\nAnomaly Detection Algorithms: Algorithms for detecting anomalies in data, such as Isolation Forests, can benefit from GPU acceleration.\nNeural Collaborative Filtering: Used in recommendation systems, this approach can be accelerated with CUDA to improve recommendation speed.\nIt\u0026rsquo;s essential to note that not all machine learning algorithms can be effectively optimized with CUDA. The feasibility of GPU acceleration depends on several factors, including the specific algorithm, the dataset size, and the availability of GPU support in the machine learning libraries or frameworks you are using. Additionally, optimizing machine learning algorithms for CUDA may require expertise in GPU programming and the use of libraries like CUDA, cuDNN, and cuBLAS to take full advantage of the GPU\u0026rsquo;s capabilities.\n"},"title":"cuda"},"/notes/ds/":{"data":{"":"","auc#AUC":"","bias-vs-variance#Bias vs Variance":"","decision-trree#Decision Trree":"","dimensionality-reduction-algorithms#Dimensionality reduction algorithms":"PCA ","ensemble-learning#Ensemble Learning":"Combined multiple weak models/learners into one predictive model to reduce bias, variance and/or improve accuracy.\nTypes of Ensemble Learning: N number of weak learners Bagging: Trains N different weak models(usually of same types - homogenous) with N non-overlapping subset of the input dataset in parallel. In the test phase, each model is evaluated. The label with the greatest number of predictions is selected as the prediction. Bagging methods reduces variance of the prediction. Simple voting Boosting: Trains N different weak models(usually of same types - homogenous) with the complete dataset in a sequential order. The datapoints wrongly classified with previous weak model is provided more weights to that they an be classified by the next weak learner properly. In the test phase, each model is evaluated and based on the test error of each weak model, the prediction is weighted for voting. Boosting methods decreases the bias of the prediction. Weighted voting Stacking: Trains N different weak models(usually of different types - heterogenous) with one of the two subsets of the dataset in parallel. Once the weak learners are trained, they are used to trained a meta learner to combine their predictions and carry out final prediction using the other subset. In the test phase, each model predicts its label, these set of labels are fed to the meta learner which generates the final prediction. Focus on improving accuracy. Learned voting(meta-learning) ","gradient-boosting-algorithm-and-adaboosting-algorithm#Gradient boosting algorithm and AdaBoosting algorithm":"","handle-imbalanced-data#Handle imbalanced data":"","k-means#K-means":"","knn#KNN":"","l1-vs-l2-regularization#L1 vs L2 regularization":"","likelihood-vs-probability#Likelihood vs Probability":"","linear-regression#Linear Regression":"","logistic-regression#Logistic Regression":"","naive-bayes-algorithm#Naive Bayes Algorithm":"","precision--recall--f1#precision \u0026amp; recall \u0026amp; F1":"","random-forest-algorithm#Random forest algorithm":"Bias vs Variance Low Bias(very sensitive to the training data), (then it performs poorly when we got new data)High Variance \u0026ndash; Overfitting\nHigher Bias(less sensitive to the training data), (then it performs better when we got new data)Low Variance \u0026ndash; Underfitting\nError = bias^2 + variance + inreducible error\nThe best model is where the error is reduced\nCompromise between bias and variance\nSolution: Use Cross Validation\nHandle imbalanced data collect more data to even the imbalances in the dataset resample the dataset to correct for imbalances try a different algorithm altogether on your dataset precision \u0026amp; recall \u0026amp; F1 Precision is a good measure to determine, when the costs of False Positive is high. We know that Recall shall be the model metric we use to select our best model when there is a high cost associated with False Negative. F1 Score might be a better measure to use if we need to seek a balance between Precision and Recall AND there is an uneven class distribution (large number of Actual Negatives). F1: weighted average of the precision and recall of a model. 1 is the best, 0 is the worst. You would use it in classification tests where true negatives don\u0026rsquo;t matter much. AUC 0.5 \u0026lt; ROC \u0026lt; 0.7: Poor discrimination 0.7 ≤ ROC \u0026lt; 0.8: Acceptable discrimination 0.8 ≤ ROC \u0026lt; 0.9: Excellent discrimination ROC ≥ 0.9: Outstanding discrimination\nRegularization Regularization is an approach to address over-fitting in ML. Overfitted model fails to generalize estimations on test data When the underlying model to be learned is low bias/high variance, or when we have small amount of data, the estimated model is prone to over-fitting. Types of Regularization 1. Modify the loss function L2 Regularization: Prevents the weights from getting too large(defined by L2 norm). Larger the weights, more complex the model is, more chances of overfitting. L1 Regularization: Prevents the weights from getting too large(defined by L1 norm). Larger the weights, more complex the model is, more chances of overfitting. L1 Regularization introduces sparsity in the weights. It forces more weights to be zero, than reducing the average magnitude of all weights. Entropy: Used for the models that output probability. Forces the probability distribution towards uniform distribution. 2. Modify data sampling Data augmentation: Create more data from available data by randomly cropping, dialting, rotating, adding small amount of noise, etc. K-fold Cross-validation: Divide the data in to k groups. Train on (k - 1) groups and test on 1 group. Try all k possible combinations. 3. Change training approach Injecting noise: Add random noise to the weights when they are being learned. It pushes the model to be relatively insensitive to small variations in the weights, hence regularization. Dropout: Generally used for neural networks. Connections between consecutive layers are randomly dropped based on a dropout-ratio and the remaining network is trained in the current iteration. In the next iteration, another set of random connections are dropped. L1 vs L2 regularization L2 regularization tends to spread error among all the terms L1 is more binary/sparse, with many variables either being assigned a 1 or 0 in weighting. L1 corresponds to setting a Laplacean prior on the terms L2 corresponds to a Gaussian prior. Type I vs Type II error Type I error is a false positive: claiming something has happened when it hasn\u0026rsquo;t e.g. Telling a man he is pregnant. Type II eeror is a false negative: claiming nothing is happening when in fact something is. e.g. Telling a pregnant woman she isn\u0026rsquo;t carrying a baby. Likelihood vs Probability Linear Regression Logistic Regression Decision Trree SVM Soft Margin The name Support Vector Classifier comes from the fact that the observations on the edge and within the Soft Margin are called Support Vectors. Naive Bayes Algorithm Why \u0026ldquo;Naive\u0026rdquo; Because it makes an assumption: the conditional probabilities is calculated as the pure product of the individual probabilities of components. This implies the absolute independence of features \u0026ndash; a condition probably never met in real life. KNN sort the nearest neighbors of the given point by the distances in increasing order K small K: low bias, high variance, overfitting large K: high bias, low variance, underfitting best K: can be found with cross validation and learning curve Classification \u0026amp; Regression Classification - vote by top k candidates Regression - average of the k nearest neighbors\u0026rsquo; labels as the prediction K-means Random forest algorithm ","regularization#Regularization":"","svm#SVM":"","type-i-vs-type-ii-error#Type I vs Type II error":""},"title":"ds"},"/notes/git/":{"data":{"":"git push --set-upstream origin branchb git branch -m \u0026lt;new name\u0026gt; echo \u0026#34;# private_notebook\u0026#34; \u0026gt;\u0026gt; README.md git init git add README.md git commit -m \u0026#34;first commit\u0026#34; git branch -M main git remote add origin git@github.com:yixianwang/private_notebook.git git push -u origin main ","semantic-commit-messages#Semantic Commit Messages":"See how a minor change to your commit message style can make you a better programmer.\nFormat: \u0026lt;type\u0026gt;(\u0026lt;scope\u0026gt;): \u0026lt;subject\u0026gt;\n\u0026lt;scope\u0026gt; is optional\nExample feat: add hat wobble ^--^ ^------------^ | | | \u0026#43;-\u0026gt; Summary in present tense. | \u0026#43;-------\u0026gt; Type: chore, docs, feat, fix, refactor, style, or test. More Examples:\nfeat: (new feature for the user, not a new feature for build script) fix: (bug fix for the user, not a fix to a build script) docs: (changes to the documentation) style: (formatting, missing semi colons, etc; no production code change) refactor: (refactoring production code, eg. renaming a variable) test: (adding missing tests, refactoring tests; no production code change) chore: (updating grunt tasks etc; no production code change) References:\nhttps://www.conventionalcommits.org/ https://seesparkbox.com/foundry/semantic_commit_messages http://karma-runner.github.io/1.0/dev/git-commit-msg.html "},"title":"git"},"/notes/hexo/":{"data":{"":"","asset-folders#Asset folders":"use hexo syntax for img: second priority _config.yml post_asset_folder: true Then next time we create new post with hexo command line, it will also create a asset folder a along with a.md\nNotice: jpg works, png not works # within a.md {% asset_img testdel.jpg Image Title Here %} {% asset_link testdel.jpg %} {% asset_path testdel.jpg %} use markdown syntax for img: first priority # _config.yml post_asset_folder: true marked: prependRoot: true postAsset: true Notice: create an additional folder for reference and convinence ![images](a/testdel.jpg) ","creating-a-theme#Creating a theme":" file layout.ejs is the overview of the structure partial partial can make process modular\ncreate partial folder, and a file header.ejs # within layout.ejs # title is the parameter \u0026lt;body\u0026gt; \u0026lt;%- parital(\u0026#39;partial/header\u0026#39;, {title: \u0026#39;red\u0026#39;}) %\u0026gt; \u0026lt;/body\u0026gt; # within partial/header.ejs # to get the parameter \u0026lt;%= title %\u0026gt; Variables ","doc#Doc":"","draft#DRAFT":"","new-blog-project#NEW BLOG PROJECT":"","official-theme#Official Theme":"Clone github to themes folder\ntheme: change the name here to theme-folder\u0026#39;s name Then restart hexo server\n","page#PAGE":"","post#POST":"","scaffolds#SCAFFOLDS":"","tag-plugins#Tag Plugins":"Code Block {% codeblock lang:c++ %} {% endcodeblock %} Youtube {% youtube AnyYoutubeID %} ","tags--categories#Tags \u0026amp; Categories":"Doc Doc Restart hexo server, each time changed yml file NEW BLOG PROJECT hexo init blog_project_name POST creating a post # under blog folder to create a new post with name a.md hexo new a DRAFT create a new draft hexo new draft b test draft hexo server --draft publish the draft # move b from _drafts folder to _posts folder hexo publish b PAGE hexo new page c to access the page c \u0026ldquo;http://localhost:4000/about/\u0026rdquo; SCAFFOLDS For handling default content\ncreate a new file within scafoolds. e.g. giraffe.md title: {{ title }} // title within curly brace here are just placeholder date: {{ date }} layout: {{ layout }} create new post with template giraffe hexo new giraffe f Tags \u0026amp; Categories Within _posts folder a.md file\n--- tags: [Tag1, Tag2, Tag3] categories: - [Cat1, Cat1.1] - [Cat2] - [Cat3] --- "},"title":"hexo"},"/notes/hugo/":{"data":{"":"","archetypes#archetypes":"","content#content":"","creating#Creating":"","installing--using-themes#Installing \u0026amp; using themes":"","installing-on-mac#Installing on mac":"","shortcodes#Shortcodes":"","tags--categories#Tags \u0026amp; Categories":"Theme Doc - hextra hextra doc Themes hextra # an easy way? hugo serve -D -t theme_name_here # Change directory to the theme folder cd hextra-starter-template # Start the server hugo mod tidy hugo server --logLevel debug --disableFastRender -p 1313 # Update the theme hugo mod get -u hugo mod tidy # start the server for draft hugo server -D Installing on mac brew install hogo Creating hugo new site first_site Installing \u0026amp; using themes # within config.toml theme = \u0026#34;the name of theme downloaded\u0026#34; content hugo new a.md List pages: if the folder is not the first folder level under content # it must to be _index.md here hugo new dir1/dir2/_index.md archetypes find if there exist the folder name within archetypes that correspond with the folder name within content yes: use the specific markdown file; no: use the default.md Shortcodes # \\{\\{\\\u0026lt; shortcode-name param1 \\\u0026gt;\\}\\} # e.g. \\{\\{\\\u0026lt; youtube AnyYoutubeID \\\u0026gt;\\}\\} Tags \u0026amp; Categories tags: [\u0026#34;tag1\u0026#34;, \u0026#34;tag2\u0026#34;, \u0026#34;tag3\u0026#34;] categories: [\u0026#34;cat1\u0026#34;] Creating taxonomy Notice: modify themes/ga-hugo-theme/layouts/_default/list.html by adding a new line for that name mood mood: [\u0026#34;happy\u0026#34;, \u0026#34;upset\u0026#34;] # hugo.toml # even if tag and category are default, but we have to include them when we creating new taxonomies # after modifyint he toml file, restart the server [taxonomies] tag = \u0026#34;tags\u0026#34; category = \u0026#34;categories\u0026#34; mood = \u0026#34;moods\u0026#34; ","theme-doc---hextra#Theme Doc - hextra":"","themes#Themes":""},"title":"hugo"},"/notes/java/":{"data":{"":"Java接口(Interface)是一系列方法的声明，是一些方法特征的集合，一个接口只有方法的特征没有方法的实现，因此这些方法可以在不同的地方被不同的类实现，而这些实现可以具有不同的行为。打一个比方，接口好比一个戏中的角色，这个角色有一些特定的属性和操作，然后实现接口的类就好比扮演这个角色的人，一个角色可以由不同的人来扮演，而不同的演员之间除了扮演一个共同的角色之外，并不要求其它的共同之处。\n接下来我们来介绍几个面试常用的Interface。\nSet Set注重独一无二,该体系集合可以知道某物是否已经存在于集合中,不会存储重复的元素。Set的实现类在面试中常用的是：HashSet 与 TreeSet\nHashSet 无重复数据 可以有空数据 数据无序 Set\u0026lt;String\u0026gt; set = new HashSet\u0026lt;\u0026gt;(); for (int i = 1; i \u0026lt; 6; i ++) { set.add(i + \u0026quot;\u0026quot;); } set.add(\u0026quot;1\u0026quot;); //不会重复写入数据 set.add(null);//可以写入空数据 Iterator\u0026lt;String\u0026gt; iter = set.iterator(); while (iter.hasNext()) { system.out.print(iter.next() + \u0026quot; \u0026quot;);//数据无序 }// 输出(无序)为 3 4 1 5 null 2 TreeSet 无重复数据 不能有空数据 数据有序 Set\u0026lt;String\u0026gt; set = new TreeSet\u0026lt;\u0026gt;(); for (int i = 1; i \u0026lt; 6; i ++) { set.add(i + \u0026quot;\u0026quot;); } set.add(\u0026quot;1\u0026quot;); //不会重复写入数据 //set.add(null);//不可以写入空数据 Iterator\u0026lt;String\u0026gt; iter = set.iterator(); while (iter.hasNext()) { system.out.print(iter.next() + \u0026quot; \u0026quot;);//数据有序 }// 输出(有序)为 1 2 3 4 5 Map Map用于存储具有映射关系的数据。Map中存了两组数据(key与value),它们都可以是任何引用类型的数据，key不能重复，我们可以通过key取到对应的value。Map的实现类在面试中常用是：HashMap 和 TreeMap.\nHashMap key 无重复，value 允许重复 允许 key 和 value 为空 数据无序 public class Solution { public static void main(String[] args){ Map\u0026lt;String, String\u0026gt; map = new HashMap\u0026lt;\u0026gt;(); for (int i = 5; i \u0026gt; 0; i --) { map.put(i + \u0026quot;\u0026quot;, i + \u0026quot;\u0026quot;); } map.put(\u0026quot;1\u0026quot;,\u0026quot;1\u0026quot;);//key无重复 map.put(\u0026quot;11\u0026quot;,\u0026quot;1\u0026quot;);//value可以重复 map.put(null, null);//可以为空 for (Iterator i = map.keySet().iterator(); i.hasNext(); ) { String key = (String)i.next(); String value = map.get(key); System.out.println(\u0026quot;key = \u0026quot; + key + \u0026quot;, value = \u0026quot; + value); } } } //输出 /* key = 11, value = 1 key = null, value = null key = 1, value = 1 key = 2, value = 2 key = 3, value = 3 key = 4, value = 4 key = 5, value = 5 */ //输出顺序与输入顺序无关 TreeMap key 无重复，value 允许重复 不允许有null 有序(存入元素的时候对元素进行自动排序，迭代输出的时候就按排序顺序输出) public class Solution { public static void main(String[] args){ Map\u0026lt;String, String\u0026gt; map = new TreeMap\u0026lt;\u0026gt;(); for (int i = 5; i \u0026gt; 0; i --) { map.put(i + \u0026quot;\u0026quot;, i + \u0026quot;\u0026quot;); } map.put(\u0026quot;1\u0026quot;,\u0026quot;1\u0026quot;);//key无重复 map.put(\u0026quot;11\u0026quot;,\u0026quot;1\u0026quot;);//value可以重复 //map.put(null, null);//不可以为空 for (Iterator i = map.keySet().iterator(); i.hasNext(); ) { String key = (String)i.next(); String value = map.get(key); System.out.println(\u0026quot;key = \u0026quot; + key + \u0026quot;, value = \u0026quot; + value); } } } //输出 /* key = 1, value = 1 key = 11, value = 1 key = 2, value = 2 key = 3, value = 3 key = 4, value = 4 key = 5, value = 5 */ //输出顺序位String排序后的顺序 List 一个 List 是一个元素有序的、可以重复(这一点与Set和Map不同)、可以为 null 的集合，List的实现类在面试中常用是：LinkedList 和 ArrayList LinkedList 基于链表实现\nArrayList 基于动态数组实现 LinkedList 与 ArrayList 对比： 对于随机访问get和set，ArrayList绝对优于LinkedList，因为LinkedList要移动指针 对于新增和删除操作add和remove，在已经得到了需要新增和删除的元素位置的前提下，LinkedList可以在O(1)的时间内删除和增加元素，而ArrayList需要移动增加或删除元素之后的所有元素的位置，时间复杂度是O(n)的，因此LinkedList优势较大 Queue 队列是一种比较重要的数据结构，它支持FIFO(First in First out)，即尾部添加、头部删除（先进队列的元素先出队列），跟我们生活中的排队类似。\nPriorityQueue 基于堆(heap)实现 非FIFO(最先出队列的是优先级最高的元素) 普通 Queue 基于链表实现 FIFO "},"title":"java"},"/notes/latex/":{"data":{"":"$\\KaTeX$ is used for rendering LaTeX math expressions. It can be enabled per page by setting math to true in the page front matter.\nMarkdown--- title: \u0026#34;My Page with LaTeX\u0026#34; math: true --- When enabled, the scripts, stylesheets and fonts from KaTeX will be included automatically in your site. You can start using LaTeX math expressions in your Markdown content.\n","chemistry#Chemistry":"Chemistry expressions are supported via mhchem extension.\nInline: $\\ce{H2O}$ is water.\nSeparate paragraph:\npage.md$$\\ce{Hg^2+ -\u0026gt;[I-] HgI2 -\u0026gt;[I-] [Hg^{II}I4]^2-}$$ $$\\ce{Hg^2+ -\u0026gt;[I-] HgI2 -\u0026gt;[I-] [Hg^{II}I4]^2-}$$\n","example#Example":"Both inline and separate paragraph LaTeX math expressions are supported in the Markdown content.\nInline page.mdThis $\\sigma(z) = \\frac{1}{1 + e^{-z}}$ is inline. This $\\sigma(z) = \\frac{1}{1 + e^{-z}}$ is inline.\nSeparate Paragraph page.md$$F(\\omega) = \\int_{-\\infty}^{\\infty} f(t) e^{-j\\omega t} \\, dt$$ will be rendered as:\n$$F(\\omega) = \\int_{-\\infty}^{\\infty} f(t) e^{-j\\omega t} , dt$$\n","supported-functions#Supported Functions":"For a list of supported functions, see KaTeX supported functions."},"title":"latex"},"/notes/notebook/":{"data":{"":"","0x3f3f3f3f--0xcfcfcfcf#0x3f3f3f3f \u0026amp;\u0026amp; 0xcfcfcfcf":"","c-concept--requires#c++ concept \u0026amp;\u0026amp; requires":" To MP3 Converter Free pandoc convert markdown to org set in c++ Enable/Disable Monitor 0x3f3f3f3f \u0026amp;\u0026amp; 0xcfcfcfcf vim Table Mode vim generate contents scp zip patch conda export dependencies from poetry to requirements.txt change poetry python version python special characters for ps1 in bashshell rsync crontab.guru pytest github remove file from staging area viewing info about the remote repository pushing changes merge a branch deleting a branch stash diffmerge add remove changes change commit message(changed commit history) add a file to the last commit(changed commit history) commited to the wrong branch undo some commit but other people have already pulled those changes itertools Sorting Lists, Tuples, and Objects Lists Objects global vs nonlocal Context Manager grep emacs vc emacs magit c++ STL c++ concept \u0026amp;\u0026amp; requires To MP3 Converter Free cat *.mp3 \u0026gt; final.mp3 # best brew install mp3wrap mp3wrap output.mp3 *.mp3 pandoc convert markdown to org pandoc -f markdown -t org -o note_dynamic_programming.org note_dynamic_programming.md set in c++ #include \u0026lt;vector\u0026gt; #include \u0026lt;numeric\u0026gt; #include \u0026lt;iostream\u0026gt; #include \u0026lt;cassert\u0026gt; #include \u0026lt;unordered_map\u0026gt; #include \u0026lt;unordered_set\u0026gt; #include \u0026lt;set\u0026gt; #include \u0026lt;iostream\u0026gt; #include \u0026lt;queue\u0026gt; #include \u0026lt;vector\u0026gt; #include \u0026lt;set\u0026gt; #include \u0026lt;map\u0026gt; #define assertm(exp, msg) assert(((void)msg, exp)) #define print(input) for (auto\u0026amp; elem : input) std::cout \u0026lt;\u0026lt; elem \u0026lt;\u0026lt; std::endl int main() { auto cmp = [](const std::pair\u0026lt;int, int\u0026gt;\u0026amp; a, const std::pair\u0026lt;int, int\u0026gt;\u0026amp; b) {return a.second \u0026lt; b.second;}; std::set\u0026lt;std::pair\u0026lt;int, int\u0026gt;, decltype(cmp)\u0026gt; heap; heap.insert(std::make_pair(1, 3)); heap.insert(std::make_pair(31, 1)); heap.insert(std::make_pair(4, 4)); heap.insert(std::make_pair(2, 2)); heap.insert(std::make_pair(5, 5)); auto it = heap.begin(); it = std::next(it, 2); it = std::prev(it, 1); std::cout \u0026lt;\u0026lt; it-\u0026gt;first \u0026lt;\u0026lt; \u0026#34; \u0026#34; \u0026lt;\u0026lt; it-\u0026gt;second \u0026lt;\u0026lt; std::endl; int index = 31; auto it2 = std::find_if(heap.begin(), heap.end(), [index](const std::pair\u0026lt;int, int\u0026gt;\u0026amp; a) {return a.first == index;}); heap.erase(it2); it = heap.begin(); std::cout \u0026lt;\u0026lt; it-\u0026gt;first \u0026lt;\u0026lt; \u0026#34; \u0026#34; \u0026lt;\u0026lt; it-\u0026gt;second \u0026lt;\u0026lt; std::endl; return 0; } Enable/Disable Monitor SwitchResX 0x3f3f3f3f \u0026amp;\u0026amp; 0xcfcfcfcf If you are using C++ to write program, sometimes you need to set an large number. We can use INT_MAX of course, however, sometimes we may get overflow if we add the large number by 1.\nSome people they like use this number as INF which is 0x3f3f3f3f. For -INF, we can use 0xcfcfcfcf\nvim Table Mode \\tm | || [|, ]|, {| \u0026amp; }| to move left | right | up | down cells i| or a| # insert a cell \\tdd # delete a row \\tdc # delete a coloumn \\tic # insert column vim generate contents :GenTocGFM Generate table of contents in GFM link style. This command is suitable for Markdown files in GitHub repositories, like README.md, and Markdown files for GitBook. :GenTocRedcarpet Generate table of contents in Redcarpet link style. This command is suitable for Jekyll or anywhere else use Redcarpet as its Markdown parser. :GenTocGitLab Generate table of contents in GitLab link style. This command is suitable for GitLab repository and wiki. :GenTocMarked Generate table of contents for iamcco/markdown-preview.vim which use Marked markdown parser. scp scp -ri /Users/yixianwang/.ssh/aws_ps.pem destination ec2-user@ec2-18-217-15-234.us-east-2.compute.amazonaws.com:~/ scp -ri /Users/yixianwang/.ssh/aws_ps.pem ec2-user@ec2-18-217-15-234.us-east-2.compute.amazonaws.com:~/ destination scp -ri /Users/yixianwang/.ssh/aws_skater.pem ubuntu@ec2-3-142-96-155.us-east-2.compute.amazonaws.com:~/project4 ~/Downloads/ zip zip -r py_image_manipulation.zip py_image_manipulation patch make clean make -f Makefile.test clean diff -ruN src src-finished \u0026gt; xv6.patch 1. Insert \u0026#34;xv6.patch\u0026#34; file in \u0026#34;src\u0026#34; folder 2. Under \u0026#34;src\u0026#34; folder, command \u0026#34;patch -i xv6.patch\u0026#34; 3. Make xv6 and run the tests conda conda create --name myenv Python=3.8 --no-default-packages conda env list conda env remove -n myenv conda install numpy [matplotlib seaborn pandas] conda list # search all versions of pandas that available to install conda search pandas conda install pandas=0.25.2 conda update pandas # remove package conda remove numpy # install pip locally with conda, inside the virtual env conda install pip export dependencies from poetry to requirements.txt python3 -m venv .venv source .venv/bin/activate poetry export --without-hashes \u0026gt; requirements.txt pip install -r requirements.txt change poetry python version poetry env use /usr/local/bin/python3.9 python python -m SimpleHTTPServer 8000 special characters for ps1 in bashshell \\h the hostname up to the first . \\n newline \\s the name of the shell \\t the current time in 24-hour format \\u the username of the current user \\w the current working direcotry \\W the basename of the current working directory rsync rsync -zaP --dry-run dir dir/ crontab.guru crontab -l crontab -e crontab -r pytest pytest --junitxml=result.xml poetry run pytest test_py_image_manipulation.py github git config --global user.name \u0026#34;firstname lastname\u0026#34; git config --global user.email \u0026#34;email@email.com\u0026#34; git config --list git help \u0026lt;verb\u0026gt; git \u0026lt;verb\u0026gt; --help git diff remove file from staging area git reset filename # remove one file git reset # remove everythin viewing info about the remote repository git remote -v git branch -a pushing changes git pull origin master git push origin master # origin: the name of remote repository. master: the branch we want to push to git branch branchname git checkout branchname git push -u origin branchname git branch -a merge a branch git checkout master git pull origin master git branch --merged git merge branchname git push origin master deleting a branch git branch --merged git branch -d branchname git branch -a git push origin --delete branchname stash git stash save \u0026#34;Worked on some function\u0026#34; git stash list git stash apply/drop stash@{0} git stash pop git stash clear # be careful here git checkout -- . diffmerge git config --global diff.tool diffmerge git config --global difftool.diffmerge.cmd \u0026#39;diffmerge \u0026#34;$LOCAL\u0026#34; \u0026#34;$REMOTE\u0026#34;\u0026#39; git config --global merge.tool diffmerge git config --global mergetool.diffmerge.cmd \u0026#39;diffmerge --merge --result=\u0026#34;$MERGED\u0026#34; \u0026#34;$LOCAL\u0026#34; \u0026#34;$(if test -f \u0026#34;$BASE\u0026#34;; then echo \u0026#34;$BASE\u0026#34;; else echo \u0026#34;$LOCAL\u0026#34;; fi)\u0026#34; \u0026#34;$REMOTE\u0026#34;\u0026#39; git config --global mergetool.diffmerge.trustExitCode true # git config --global mergetool.keepBackup false git diff # old git difftool git merge branchname git mergetool git commit add Ignore the deleted files in git version 2\ngit add --no-all sub_dir/ Ignore the untracked files\ngit add -u/--update remove changes Remove changes of a file\ngit checkout filename change commit message(changed commit history) git commit --amend -m \u0026#34;new message here\u0026#34; add a file to the last commit(changed commit history) git commit --amend :wq git log --stat commited to the wrong branch move commit between branch cherry-pick creates a new commit based off our original(doesn\u0026rsquo;t delete)\ngit log # copy the hash git checkout branchname git cherry-pick #hash git checkout master remove the master commit\ngit reset soft: set back to the commit that we specified but it will keep our changes that we\u0026rsquo;ve made in the staging directory git reset --soft #the initial commit hash git reset mixed(default): keep the changes in the working directory instead of staging area git reset #the hash git reset hard: make all of our tracked files match the state that they were in at the hash we specified(leave the untracked file alone) git reset --hard #the initial commit hash remove the untracked directories and files\ngit clean -df recover from the hard reset\ngit reflog git checkout #hash before the reset git log # to check whether the commit exists git branch backup git branch # to see all branches undo some commit but other people have already pulled those changes revert: creates a new commit to reverse the effect of some ealier commits(won\u0026rsquo;t rewrite history) it\u0026rsquo;s not going to modify or delete our existing commits\ngit log git revert #hash of the commit need to be covered git diff #src #desc\nitertools # count list(zip(itertools.count(), data)) counter = itertools.count() counter = itertools.count(start=5, step=-2.5) print(next(counter)) # zip_longest vs zip data = [1, 2, 3, 4] result = list(zip(range(10), data)) result = list(itertools.zip_longest(range(10), data)) # cycle counter = itertools.cycle([1,2,3]) counter = itertools.cycle((\u0026#34;On\u0026#34;, \u0026#34;Off\u0026#34;)) print(next(counter)) # repeat counter = itertools.repeat(2) counter = itertools.repeat(2, times=3) print(next(counter)) # startmap vs map squares = map(pow, range(10), itertools.repeat(2)) # take iterables print(list(squares)) squares = map(pow, [(0, 2), (1, 2), (2, 2)]) # take paired tuples print(list(squares)) # combinations vs permutations itertools.combinations_with_replacement(list1, 2) itertools.combinations(list1, 2) itertools.permuations(list1, 2) itertools.product(list1, repeat=4) # chain itertools.chain(list1, list2, list3, ...) # isclice itertools.islice(range(10), 5) # return the first 5 elements of the iterable itertools.islice(range(10), 1, 5) # return the [1, 5) elements of the iterable itertools.islice(range(10), 1, 5, 2) # step 2 with open(\u0026#34;test.log\u0026#34;, \u0026#39;r\u0026#39;) as file: header = itertools.islice(file, 3) for line in header: print(line, end=\u0026#39;\u0026#39;) # compress vs filter itertools.compress(letters, selectors) filter(lt_2, numbers) itertools.filterfalse(lt_2, numbers) itertools.dropwhile(lt_2, numbers) itertools.takewhile(lt_2, numbers) # accumulate itertools.accumulate(numbers) # default is add operation import operator itertools.accumulate(numbers, operator.mul) # group by # note: people needs to be sorted beforehand people = [ { \u0026#39;name\u0026#39;: \u0026#39;John Doe\u0026#39;, \u0026#39;city\u0026#39;: \u0026#39;Gotham\u0026#39;, \u0026#39;state\u0026#39;: \u0026#39;NY\u0026#39; }, { \u0026#39;name\u0026#39;: \u0026#39;Jane Doe\u0026#39;, \u0026#39;city\u0026#39;: \u0026#39;Kings Landing\u0026#39;, \u0026#39;state\u0026#39;: \u0026#39;NY\u0026#39; }, ] def get_state(people): return people[\u0026#39;state\u0026#39;] people_group = itertools.groupby(people, get_state) for key, group in people_group: print(key) for person in group: print(person) print() # ? copy1, copy2 = itertools.tee(person_group) Sorting Lists, Tuples, and Objects Lists sort function is more flexible\nli = [9, 1, 8, 2, 7] s_li = sorted(li) s_tu = sorted(tu) s_di = sorted(di) s_li = sorted(li, reverse=True) print(\u0026#34;sorted function\u0026#34;, s_li) li.sort() li.sort(reverse=True) print(\u0026#34;sorted method\u0026#34;, li) sort on abs value\nli = [-6, -5, -4, 1, 2, 3] s_li = sorted(li, key=abs) print(s_li) Objects class Employee(): def __init__(self, name, age, salary): self.name = name self.age = age self.salary = salary def __repr__(self): return f\u0026#34;({self.name}, {self.age}, {self.salary})\u0026#34; e1 = Employee(\u0026#34;Carl\u0026#34;, 37, 2000) e2 = Employee(\u0026#34;Sarah\u0026#34;, 23, 10000) e3 = Employee(\u0026#34;John\u0026#34;, 77, 300) employees = [e1, e2, e3] # customize key function def e_sort(emp): return emp.name s_employees = sorted(employees, key=e_sort, reverse=True) # lambda function s_employees = sorted(employees, key=lambda e: e.name) # attrgetter from operator import attrgetter s_employees = sorted(employees, key=attrgetter(\u0026#39;age\u0026#39;)) global vs nonlocal LEGB\nLocal, Enclosing, Glboal, Built-in\nContext Manager import os from contxtlib import contextmanager cwd = os.getcwd() os.chdir(\u0026#34;Sample-dir-one\u0026#34;) print(os.listdir()) os.chdir(cwd) cwd = os.getcwd() os.chdir(\u0026#34;Sample-dir-two\u0026#34;) print(os.listdir()) os.chdir(cwd) @contextmanager def change_dir(destination): try: cwd = os.getcwd() os.chdir(destination) yield finally: os.chdir(cwd) with change_dir(\u0026#34;Sample-dir-one\u0026#34;): print(os.listdir()) with change_dir(\u0026#34;Sample-dir-two\u0026#34;): print(os.listdir()) grep grep \u0026#34;Yixian\u0026#34; name.txt grep -w \u0026#34;Yixian\u0026#34; name.txt grep -wi \u0026#34;Yixian\u0026#34; name.txt grep -win \u0026#34;Yixian\u0026#34; name.txt grep -win -B 4 \u0026#34;Yixian\u0026#34; name.txt grep -win -A 4 \u0026#34;Yixian\u0026#34; name.txt grep -win -C 2 \u0026#34;Yixian\u0026#34; name.txt grep -win \u0026#34;Yixian\u0026#34; ./*.txt grep -winr \u0026#34;Yixian\u0026#34; . grep -wirl \u0026#34;Yixian\u0026#34; . grep -wirc \u0026#34;Yixian\u0026#34; . history | grep \u0026#34;git commit\u0026#34; history | grep \u0026#34;git commit\u0026#34; | grep \u0026#34;dotfile\u0026#34; grep \u0026#34;...-...-....\u0026#34; phonenumber.txt # Mac egrep \u0026#34;\\d{3}-\\d{3}-\\d{4}\u0026#34; name.txt # Linux grep -P \u0026#34;\\d{3}-\\d{3}-\\d{4}\u0026#34; name.txt emacs vc c-x v // show cv options c-x vv // next action c-x vL // log emacs magit c-x m-g // show magit options c++ STL // reverse a string std::reverse(s.begin(), s.end()); // to lower case std::transform( std::begin(s), std::end(s), std::begin(s), ::tolower ); // left part are all even numbers, right part are all odd numbers std::partition( nums.begin(), nums.end(), [](auto e) { return e % 2 == 0; } ); // move all 0\u0026#39;s to the end while maintaining the relative order of non-zero elements std::stable_partition( nums.begin(), nums.end(), [](auto e) { return e % 2 != 0; } ); // std::sort -\u0026gt; std::partial_sort -\u0026gt; std::nth_element // kClosest -- version 1 std::vector\u0026lt;std::vector\u0026lt;int\u0026gt;\u0026gt; kClosest(std::vector\u0026lt;std::vector\u0026lt;int\u0026gt;\u0026gt;\u0026amp; points, int K) { std::sort( points.begin(), points.end(), [](auto const\u0026amp; a, auto const\u0026amp; b) { return std::sqrt(a[0] * a[0] + a[1] * a[1]) \u0026lt; std::sqrt(b[0] * b[0] + b[1] * b[1]); // return a[0] * a[0] + a[1] * a[1] \u0026lt; b[0] * b[0] + b[1] * b[1]; // better performance } ); return std::vector(points.begin(), points.begin() + K); } // kClosest -- version 2 \u0026amp; 3 std::vector\u0026lt;std::vector\u0026lt;int\u0026gt;\u0026gt; kClosest(std::vector\u0026lt;std::vector\u0026lt;int\u0026gt;\u0026gt;\u0026amp; points, int K) { std::partial_sort( // std::nth_element( is the partial_sort give top K elements but not in sorted order points.begin(), points.begin() + K, points.end(), [](auto const\u0026amp; a, auto const\u0026amp; b) { return a[0] * a[0] + a[1] * a[1] \u0026lt; b[0] * b[0] + b[1] * b[1]; } ); return std::vector(points.begin(), points.begin() + K); } // squares of a sorted array std::transform( A.begin(), A.end(), A.begin(), [] (auto e) { return e * e; } ); std::sort( A.begin(), A.end() ); c++ concept \u0026amp;\u0026amp; requires The add() on line 8 is consuming a named concept, Number, using the requires clause. It takes two numbers as a parameter, which should be either integer or floating_point, and returns the sum of both numbers.\nOn line 16, another function, add2(), is defined, which takes two numbers as parameters and returns the sum, but uses an unnamed concept through the requires clause.\n#include \u0026lt;iostream\u0026gt; #include \u0026lt;concepts\u0026gt; template \u0026lt;typename T\u0026gt; concept Number = std::integral\u0026lt;T\u0026gt; || std::floating_point\u0026lt;T\u0026gt;; template \u0026lt;typename T, typename U\u0026gt; requires Number\u0026lt;T\u0026gt; \u0026amp;\u0026amp; Number\u0026lt;U\u0026gt; auto add(T a, U b) { return a+b; } template \u0026lt;typename T, typename U\u0026gt; requires std::integral\u0026lt;T\u0026gt; || std::floating_point\u0026lt;T\u0026gt; \u0026amp;\u0026amp;\\ std::integral\u0026lt;U\u0026gt; || std::floating_point\u0026lt;U\u0026gt; auto add2(T a, U b) { return a+b; } int main() { std::cout\u0026lt;\u0026lt;add(5,42.1f)\u0026lt;\u0026lt;\u0026#39;\\n\u0026#39;; std::cout\u0026lt;\u0026lt;add2(42.1f,5)\u0026lt;\u0026lt;\u0026#39;\\n\u0026#39;; return 0; } ","c-stl#c++ STL":"","change-poetry-python-version#change poetry python version":"","conda#conda":"","context-manager#Context Manager":"","crontabguru#crontab.guru":"","emacs-magit#emacs magit":"","emacs-vc#emacs vc":"","enabledisable-monitor#Enable/Disable Monitor":"","export-dependencies-from-poetry-to-requirementstxt#export dependencies from poetry to requirements.txt":"","github#github":"","global-vs-nonlocal#global vs nonlocal":"","grep#grep":"","itertools#itertools":"","objects#Objects":"","pandoc-convert-markdown-to-org#pandoc convert markdown to org":"","patch#patch":"","pytest#pytest":"","python#python":"","rsync#rsync":"","scp#scp":"","set-in-c#set in c++":"","sorting-lists-tuples-and-objects#Sorting Lists, Tuples, and Objects":"","special-characters-for-ps1-in-bashshell#special characters for ps1 in bashshell":"","to-mp3-converter-free#To MP3 Converter Free":"","vim-generate-contents#vim generate contents":"","vim-table-mode#vim Table Mode":"","zip#zip":""},"title":"notebook"},"/notes/pytest/":{"data":{"":"","all-test-function-should-named-leading-with-test_#all test function should named leading with \u003ccode\u003etest_\u003c/code\u003e":"all test function should named leading with test_ def test_fun1(): assert 1 == 3 ","run-pytest#run pytest":"python -m pytest XXX.py "},"title":"pytest"},"/notes/python_script/":{"data":{"":"","add-sentences-to-the-beginning-of-all-markdown-files#Add sentences to the beginning of all markdown files":"import os # Define the sentence you want to add sentence_to_add = \u0026#34;\u0026#34;\u0026#34;+++ title = \u0026#39;Markdown\u0026#39; date = 2023-10-23T21:50:46-04:00 draft = true +++ \\n\u0026#34;\u0026#34;\u0026#34; # Specify the directory where your Markdown files are located markdown_directory = \u0026#34;./\u0026#34; # List all the Markdown files in the directory markdown_files = [f for f in os.listdir(markdown_directory) if f.endswith(\u0026#34;.md\u0026#34;)] # Loop through each Markdown file and add the sentence for markdown_file in markdown_files: file_path = os.path.join(markdown_directory, markdown_file) # Open the file in read mode to read its content with open(file_path, \u0026#39;r\u0026#39;) as file: content = file.read() # Open the file in write mode to add the sentence at the beginning with open(file_path, \u0026#39;w\u0026#39;) as file: file.write(sentence_to_add) file.write(content) print(\u0026#34;Sentences added to Markdown files.\u0026#34;) "},"title":"python_script"},"/projects/android_business_management/":{"data":{"":"","apis--android-features#APIs \u0026amp; Android Features":"","app-description#App Description":"","backend-processing-logic#Backend Processing Logic":" I have three tables in the firebase database: “allAppointments,” “allContracts,” and “allHouses.”\nThe first table, “allAppointments” stores all appointments, including past and upcoming appointments.\nThe second table, “allContracts” stores all contracts, including old and current underlying contracts.\nThe third table, “allHouses” handles the housing inventory.\nI have described how these three tables work with UI in section 6 of this report.\n","built--settings#Built \u0026amp; Settings":"","firebase-database-schemas#Firebase Database Schemas":" ","third-party-libraries--services-description#Third-party Libraries \u0026amp; Services Description":" Designed the app for my parents’ real daily business Features: cloud-hosted database, interactive charts, automatic accountant, and PDF generator Backend jobs: firebase for authentication, firestore for data and SQL query Architecture \u0026amp; Language: MVVM, Kotlin Third-party libraries: MPAndroid, Itextpdf, Android-pdf-viewer Built \u0026amp; Settings Hardware: Pixel 6 API 32 The login email: fake@example.com The password: 123456 App Description It is a business management app designed for my parents\u0026rsquo; real daily business of renting houses. It consists of two essential parts, business management and accounting. The inspiration is only from my parent\u0026rsquo;s business which is handled by their handwriting right now. This app will save them much time creating a new contract or appointment. It can also automatically create an accountant summary and some beautiful graphs simultaneously that give my parents an intuitive view of how the business is running. The firebase is heavily used in this project. APIs \u0026amp; Android Features APIs: Firebase for authentication Firestore for data backend and SQL query MPAndroid for graph PDF Generator PDF Viewer PDF Sharing Android features: RecyclerView and Adapter Fragment Intent Coroutines CardView TableView Action Bar Date Picker Bottom Navigation LiveData Third-party Libraries \u0026amp; Services Description MPAndroidChart: I use this library to create beautiful interactive pie charts, line charts, and bar charts. The good thing is that I can choose many different charts and customize them. The challenging part is customizing the x-axis because, from its last GitHub update change, the x-axis data only support float data type. That means I always have to use boilerplate code to customize and cast type in the helper function when using the customized x-axis.\nItextpdf: I use this library to customize pdf from the data drawn from firebase. The good thing is that the workflow with it is very straightforward. The challenging part is when I want to add more details to it. The serialized workflow of this library is challenging for adding multiple features or images to the pdf because the previous work can be affected by the latter added features, which might extend pdf page boundaries.\nAndroid-pdf-viewer: I can use this to view pdf smoothly. It provides an efficient pdf viewer service but does not support viewing pdf with other apps.\nFirebase: I use this to utilize firebase to store the data and keep them saved and stored in the cloud.\n","uiuxdisplay#UI/UX/Display":" For the “Contract” and “Appointment” parts, I use RecyclerView with CardView for each row. There is an insert button on both of them to insert new data into “Contract” or “Appointment.”\nFor the “Home,” there is a manage button on the top left side. We can use it to manage the housing inventory. I also want to mention that only houses in the inventory can be inserted into our “Contract” or “Appointment” list when inserting a new item.\nFor \u0026ldquo;Accountant,\u0026rdquo; this part is automatically updated with the data in firebase. In addition, TableView has a summary of the current month\u0026rsquo;s and last month\u0026rsquo;s income. It is another essential feature to reduce my parents\u0026rsquo; workload when any of the homeowners want to cash out anytime. I also added three pdf-related features here: creating, viewing, and sharing pdf.\nFor “Data,” this part is also automatically updated. Here consists of three valuable graphs to give an intuitive and straightforward overlook of how my parents’ business is running.\n"},"title":"Android App: Business Management \u0026 Accountant"},"/projects/cuda_kmeans/":{"data":{"":"","analyze-the-fraction-of-the-end-to-end-runtime-of-cuda-implementations-spent-in-data-transfer#Analyze the Fraction of the End-to-End Runtime of CUDA Implementations Spent in Data Transfer":" For both CUDA versions (Shared Memory and Basic), the fraction is decreased with the increasing input data size. This indicates that the impact of the overhead of transferring data is decreasing with the increase of input size. (Fig. 3.)\nFor CUDA Shared Memory has slightly more overhead on transferring data than CUDA basic, because to use shared memory in GPU, it must transfer data from device to shared memory whereas the CUDA Basic only has overhead on transferring data between host and device, it doesn’t need shared memory.\n","analyze-why-thrust-is-compromising#Analyze Why Thrust Is Compromising":" The Thrust is the slowest parallel implementation, and it does match my expectations. (Fig. 1, left and Fig. 2.)\nThe first reason is that the advantages of Thrust are abstraction and portability, but under the hood, it has more temporary memory allocations required by the Thrust algorithm during computing than CUDA implementations. For example, in lab2, the Thrust algorithm has additional temporary memory to store the data structure to perform the “reduce_by_key” function. However, we can visually manage every memory in CUDA implementation. Thus, these additional temporary memory allocations can impact Thrust performance.\nThe second reason is that Thrust cannot make use of shared memory or constant memory in GPU, both two kinds of memories have almost register speed. The Thrust only can make use of global GPU memory and transfer data to computing units through L1 memory to register.\nFig. 3. The fraction of the end-to-end runtime in CUDA versions(Shared Memory and Basic) is spent in data transfer.\n","compare-and-analyze-performance-among-four-implementations#Compare and Analyze Performance Among Four Implementations":" The CUDA Shared Memory has the smallest elapsed time per iteration (Fig. 1, left) but it also has the largest overhead on transferring data between host and device and between the device and shared memory. It is the fastest implementation when the input size is very large. As the amount of input data increases, it can get more benefit from parallelism and the influence of the overhead can be ignored. (Fig. 1, right)\nThe CUDA Basic is slightly slower than CUDA Shared Memory on converging speed (Fig. 1, left) and it also has a slightly smaller overhead on transferring data than CUDA Shared Memory, because it doesn’t have to transfer data between the device and shared memory. (Fig. 1, right)\nThe Thrust is always converging slower than CUDA Basic and has a similar overhead as CUDA Basic. However, it can always converge faster than Sequential implementation. (Fig. 1, left)\nThe sequential has the slowest converge speed but it doesn’t have any overhead on transferring data. The CPU is very powerful the Sequential version is the fastest implementation for the total elapsed time when the input size is small. (Fig. 1, right)\nFig. 1. Averaged elapsed time per iteration and Total elapsed time(E2E runtime) measured on different input size.\n","data#Data":" Data1: size = 2048, dim = 16, c = 16 Data2: size = 16384, dim = 24, c = 16 Data3: size = 65536, dim = 32, c = 16 ","estimate-the-best-case-performance-of-cuda-implementation-should-have-based-on-the-hardware#Estimate the best-case performance of CUDA implementation should have based on the hardware":" Based on the number of threads in my program and the number of processing contexts actually supported by my hardware\nBecause a higher occupancy reduces processor idle time(SM may stall due to unavailability of data or busy functional units) and improves overall performance. The best case of performance of CUDA implementation should have the highest occupancy in theory.\nAt first, I should list my device info in detail and then use this info to deduce the theoretical best performance of CUDA implementation speedup compared to sequential implementation. I use Codio environment for my lab2. In the Codio environment, the device is Tesla T4, the warp size is 32, the maximum number of threads per block is 1024, the maximum number of blocks is 2147483647. I also found out the bandwidth between Host and Device is 6.3GB/s and the bandwidth between Device to Device is 239.4 GB/s.\nSecondly, I ignore the time cost on data transfer and only focus on elapsed time per iteration when estimating the best-case performance, because, from the device info above, we can see that the bandwidth are too big to measure its performance and the “cudaMemCpy” instruction is not an asynchronous instruction(it will cause different performance each time when I test it because it has “barrier” within this kind of function). In addition, the fraction of the end-to-end runtime in CUDA versions is spent in data transfer are the most time-consuming part(Fig. 3.) and the fraction also decreasing when the input size increasing, this is irrelevant with the number of thread or other settings in the program. For these three reasons, it’s fair to ignore the data transfer part when estimating the best performance case. I also assume that the CPU clock cycle per processor is the same as the GPU.\nIn addition, in my program, I set my number of threads per block in my CUDA implementation to 32, because the warp size of Tesla T4 is 32, and thus the warp schedulers can map each thread to their own position without more or less. Another reason I set the block size to 32 is that after I experimented with the different thread numbers from 16 to 1024 for each given input size and I always got the smallest elapsed time per iteration when the number of threads per block is 32. (I also got the best performance when thread number is 86, the only reason I can come up with is the randomness in bank conflict, because I use double data type throughout in my program, this will produce 2-way bank conflict in 32 warp size situation, every second bank is being asked for 2 different values. The warp scheduler will handle the 2-way bank conflict in random in a sequential style.)\nAt last, from all above assumption, the theoretical speedup of my CUDA implementations should have when I set my number of threads in each block to 32, and the hardware in Codio support this setup when the input size smaller than 2147483647 * 32, which is maximum block size times warp size of Tesla T4. In this setup, the warp scheduler won’t let any memory wait for others and thus it has the maximum occupancy.\nFig. 2. Cuda Shmem, Cuda basic, and Thrust implementations speed up X times compared to sequential kmeans. “2k, 16k and 65k” means “2048, 16384 and 65536” input size. Red color is Cuda Shmem, orange color is Cuda basic and yellow color is Thrust implementation.\n","hardware-details--os-version--settings#Hardware Details \u0026amp; OS Version \u0026amp; Settings":" Implemented base sequential version of K-Means algorithm on CPU by C++ Implemented first parallel version of K-Means with Thrust primitives and a GroupBy-Aggregate by C++ Implemented second parallel version of K-Means with CUDA by C++ Implemented third parallel version of K-Means with CUDA on Shared Memory by C++ Analyzed speedup among all the implementations Hardware Details \u0026amp; OS Version \u0026amp; Settings GPU: the Codio environment(Tesla T4, Driver Version: 460.91.03 , CUDA Version: 11.2) CPU: the Codio environment(Intel(R) Xeon(R) Platinum 8259CL CPU @ 2.50GHz, 4 cores) OS: the Codio environment(Ubuntu 18.04.2 LTS (GNU/Linux 5.4.0-1054-aws x86_64)) the data in this report was collected when I set “threshold argument” to 1e-6.\n","the-fastest-implementation#The Fastest Implementation":" For averaged elapsed time per iteration, CUDA Shared Memory is always the fastest implementation on different input sizes. (Fig. 1, left) Because the shared memory in each block is very fast, which means the processing unit for each thread can access data in shared memory very fast after transferring data from global memory to the shared memory.\nFor total elapsed time (E2E runtime), sequential version is the fastest when the input size is very small and CUDA Shared Memory is the fastest when the input size is very large. (Fig. 1, right) The main reason is that the CPU is very powerful in computing a single thread and the CPU doesn’t need to transfer data between host and device. When the input size is very small, it is the fastest. However, when the input size is large, the power and advantage of parallel are obvious, and the overhead over transferring data can be ignored, and in this case, the CUDA Shared Memory is my fastest implementation.\n"},"title":"Exploring GPU Programming with CUDA/CUDA Shared Memory/Thrust Solving K-Means Algorithm"},"/projects/cuda_svm/":{"data":{"":"Implemented base sequential version of SVM algorithm on CPU by C++\n"},"title":"Exploring GPU Programming with CUDA/CUDA Shared Memory Solving SVM Algorithm"},"/projects/go_tree_comparison/":{"data":{"":"","analyze-averaged-comparetreetime#Analyze Averaged CompareTreeTime":" Fig. 3. Averaged compareTreeTime for different input file. There are three different implementation for each input file: Sequential, Goroutine per BST and Concurrent Buffer. All computing time measured in seconds.\nThe above graph shows the compareTreeTime for each input file on different implementations. When comparing the performance of different implementations, the Go per BST has better performance than the Concurrent Buffer, and the Go per BST implementation also has less complexity than the Concurrent Buffer implementation. The main reason is that when we explore concurrent buffer implementation we have to contain mutex to prevent concurrency errors and it also has additional conditional variables to deal with the buffer size. And thus the Go per BST has better performance and less complexity and less overhead.\nEven if the Concurrent buffer is slower than the Go per BST, it still has better performance than a single thread. The Concurrent buffer is 2X faster than single thread implementation and the Go per BST is 2.3X faster than single thread implementation in the “coarse.txt” case. In the “fine.txt” case, the Concurrent buffer is 1.3X faster than single thread implementation and the Go per BST has 2.8X faster than single thread implementation. It is not worth managing the thread pool, because the go can handle the goroutines correctly most of the time if we give a reasonable number of goroutines in our program. In addition, managing the thread pool also increases the complexity and overhead of our program.\nAt last, I spent about 11 days in this lab. I don’t have Golang experience before, and thus I spent 3 days on Golang syntax and concurrent knowledge and the other 8 days to work out the result and the report. Although I spent tons of time on this lab, I like the experience in this lab, because I learned a lot from it, like how to debugging and how to manage goroutines and channel correctly, etc. Thanks!\n","analyze-averaged-hashgrouptime#Analyze Averaged HashGroupTime":" Fig. 2. Averaged hashGroupTime for different input file. There are four different implementations tested on each input file: Sequential version, Channel version, Lock version and Extra Implementation for extra credit. All computing time are measured in seconds.\nIn the “simple.txt” scenario, the sequential version implementation is the fastest. In both the “coarse.txt” and “fine.txt” scenarios, the Channel version implementation is the fastest. The main reason is that when the input size is small, the concurrency mechanism has the drawback that it has to communicate between other goroutines when modifying shared data and in this scenario, this has a significant influence. However, when the input size is large like in the “coarse.txt” case which almost doesn’t have a high amount of overhead, we can see both the Channel version and Lock version can have better performance than the Sequential version and Channel version is the fastest.\nIn addition, when it comes to the “fine.txt” case, the program also has better performance on the Channel version and the Lock version implementation than the Sequential version, this indicates that our program can scale great in both cases and Channel also is the fastest.\nThe third implementation has more overhead because other threads must wait for the lock to unlock, and in this trying to unlock process, increases the complexity and overhead. We have to handle the lock and unlock procedure by ourselves.\nIn the “coarse.txt” case, the Channel version is 2.5X faster than a single thread, and the Lock version is 2.4X faster than a single thread. In addition, the channel implementation is much simpler, the main reason is that I don’t have to worry about the lock, the channel is threaded safe and only one channel changes the data at a time. Furthermore, this result in the channel version has a much simpler implementation, because the channel implicitly handles the lock mechanism, and thus the channel in Go is threaded safe. We don’t have to bother with lock explicitly and this decreases lots of complexity.\nFor extra credits, I also implement fine-grain synchronization to allow up to data-workers threads to access the data structure at once. It was not access to the shared data structure a bottleneck before. The main reason is that even if it has more parallelism it also has more overhead and it also has to wait for other threads to unlock the shared data structure. These extra implementations also are not simpler than the previous three implementations, because we have to handle the different threads to access data and avoid concurrency errors.\n","analyze-hashtime#Analyze HashTime":" Fig. 1. Hash value computing time compared to Goroutines. Different input files has different hash-workers experiments. Computing time are all measured in seconds. The x-axis represents the number of hash-workers.\nThe above graph shows the computing time for each BST hashes. The red circle indicates the number of hash-workers equal to the input size for a specific input file, for example, “simple.txt” has an input size of 12. When the input size is small, the sequential version is the fastest. The main reason is that our CPU can solve the problem very fast in sequential when the input size is small and the overhead of the data communication in parallel also influence the performance, and thus in small input size case, the sequential version is the fastest. In a small input size case, the sequential version is about 5X times faster than my second implementation that iterates over the available BSTs. However, when the input size increases and the input dimension increases, we can see from the above graph that the program can get benefit from Goroutine implementations compared to the sequential version.\nMy second implementation has better performance than my first implementation in “coarse.txt” and “fine.txt” cases. The main reason is that my laptop has 4 core CPUs, when the number of goroutines reaches the number of cores of my laptop, it can map different jobs to different CPU processors. As the second graph shows, the benefits of parallelism without high amounts of overhead have significant improvement when the number of hash-workers is 4. The third graph above has the same trend when the number of hash-worker is 4. My second implementation performs about 4X times better than my first implementation in both these cases.\nGo can manage goroutines very well. The main reason is that goroutines have growable segmented stacks and they grows as needed, and the Go runtime does the scheduling, not the OS.\nHowever, I still think we have to worry about or pay attention to the number of goroutines because when we know the hardware, we can get the best performance for our program.\nIf the number of goroutines is less than the number of CPU cores, the program cannot get the best performance on specific hardware even if the Go runtime can schedule the goroutine very well. If the number of goroutines is much more than the number of CPU cores, it will also have some overhead when creating the segment stack even if only 3 registers need when goroutine switch context, and thus I think we also can’t get ideal performance on this setup. In both “coarse.txt” and “fine.txt” cases, I can get the best performance when my second implementation has 4 hash-workers which is the number of the CPU cores and it can also get ideal performance when the number of hash-workers is in a reasonable range. So I think this is kind of proof.\n","data#Data":" Data1: simple Data2: coarse Data3: fine ","hardware-details--os-version#Hardware Details \u0026amp; OS Version":" Implemented concurrency programming model to compute BST(binary search tree) equivalence with Go Implemented channels, go-routines, and signaling with Go Programmed threads to parallelize hash operations with Go Assembled a concurrent buffer to secure communication among threads Analyzed performance among all the implementations Hardware Details \u0026amp; OS Version CPU: 2 GHz Quad-Core Intel Core i5 OS: macOS Big Sur version 11.6 "},"title":"Exploring Concurrency Programming with Go"},"/projects/mpi_barnes_hut/":{"data":{"":"","analyze#Analyze":" My approach can be represented by two main parts: the first is constructing a sequential version of the Barnes-Hut Algorithm and implementing OpenGL to visualize the result, the second is implementing API of MPI to take advantage of parallel from different processes.\nIn my second part, I tried to implement the method introduced by Grama in the reference paper which is “Scalable Parallel Formulations of the Barnes-Hut Method for n-Body Simulations”, however, in my implementation, after each process computes the trees locally, I cannot get the ideal performance on merging trees part. The paper said the only information that needs to be communicated for merging trees is the number of particles and the center of mass. From my understanding I need to construct a parent node above previous processes, however, I found out that I need to construct the whole new tree in the parent process and other processes need to send the nodes messages of the local tree to parent process because different processes do not share memories and after “Allgather” method, then other processes can traverse the tree during force computing part. These messages passing in tree merging impact performance dramatically in my application and thus in my final submission I didn’t utilize the tree merging method introduced by Grama in the paper.\nHowever, I implement force computation with MPI and get an obvious performance boost when running my program in multiple processes scenarios. I split input nodes which used to construct the tree into #(number of inputs/number of processes) parts. I set the last rank to be the root process that implements “Gather” and “Scatter” method because, in this setting, it can handle the situation that processes cannot split input nodes equally. And the root process which implements the “Gather” and “Scatter” method does not need to pay attention to the size of messages, because the unevenly be separated input nodes will within the last rank which is the root process.\nFig. 1. Performance measurement by fixing the number of steps(-s parameter to 1000), the number of threshold for MAC(-t parameter to 1.0) and the timestep(-d parameter to 0.04).\nIn Fig. 1., I measured the performance of my program from different input sizes by fixing other parameters. In a small input size case(“nb-10.txt”), my program cannot take advantage of the parallel and it can get the best performance when the number of processes is one that is sequentially implemented as shown in the first graph in Fig 1. The main reason is that in a modern computer, the CPU is very powerful it can handle instructions very fast in a sequential style in small input size case. In addition, the small input size will have more overhead on message passing if the number of processes is greater than one which is parallelly implemented. Thus when I increase the number of processes, it will have more overhead on message passing and it will impact the performance of the program. In the (“nb-100.txt”) input scenario, our program can get benefit from parallel. It can get the best performance when the number of processes is around 10 shown as in the second graph in Fig 1, the main reason is that my processor has 10 cores and thus different cores in the processor can handle different processes separately. When I increase the number of processes from 1 to 10, the performance is improved significantly, this indicates that we can get benefit from the computing force in a parallel style. However, when the number of processes is greater than 10, the performance is decreased, some processes are idle during the run time because all 10 cores in processor work on 10 processes separately and the scheduler will let other processes wait for these 10 processes. In a large input size case(“nb-100000.txt”) case, my program has similar improvement when increasing the number of processes. It also can get benefit from the parallel and it can get best performance when the number of processes is around five. However, the performance decreased earlier than the (“nb-100.txt”) case, the main reason is that a large input size has more overhead on the communication between processes during computing force. It has to gather all data computed in local processes in each iteration step and thus the running time in large input size case increasing earlier than previous case. Fig. 2. Average running time by fixing the number of processes(-np parameter to 4), the number of steps(-s parameter to 1000) and the timestep(-d parameter to 0.04). NAÏVE is the quadratic relationship.\nThe running time results in Fig 2 indicates that the Barnes-Hut approximation can significantly speed up computation about 2.5X when theta is setting to 1.0 or 1.5 comparing to the NAÏVE case. As expected, the naïve approach exhibits a quadratic relationship, whereas increasing the theta parameter leads to faster calculations. It does not fare better than the naïve approach until processing the largest input file. Until that point, the overhead of quadtree construction and center of mass calculation outstrips any gains in force estimation. For theta=1 and theta=1.5, however, we see a significant improvement in running time, with similar performance for each. ","data#Data":" Data1: nb-10.txt Data1: nb-100.txt Data1: nb-100000.txt ","hardware-details--os-version#Hardware Details \u0026amp; OS Version":" Implemented astrophysical simulation solved N-body problem using Barnes-Hut algorithm with MPICH by C++ Programmed OpenGL to visualize the movement of the bodies in the domain by C++ Analyzed performance with the number of bodies, processors, timesteps, iterations Hardware Details \u0026amp; OS Version Processor: 3.6GHz 10-core Intel Core i9 Memory: 32GB 2667 MHz DDR4 OS version: macOS Big Sur Version 11.6.1 "},"title":"Exploring Parallel Processes Programming with MPICH Simulating Barnes Hut Algorithm"},"/projects/nlp_semantic_parsing_encoder_decoder/":{"data":{"":" Implemented an Encoder-Decoder model for semantic parsing with Pytorch Implemented a decoder by using LTSM whose output is passed to a feedforward layer and a softmax over the vocabulary Added attention mechanisms to the model to make it more powerful and faster to train "},"title":"Semantic Parsing with Encoder-Decoder Models"},"/projects/pthreads_prefix_sum/":{"data":{"":" Implemented base sequential version of work-efficient parallel prefix sum algorithm by C++ Implemented parallel and barrier versions of work-efficient parallel prefix sum with POSIX thread (pthread) by C++ Analyzed speedup among the all implantations with respect to the number of threads and data size ","abstract#Abstract":" I use Work Efficient Algorithm with building blocks style to compute prefix sum of large array. My work-efficient algorithm has O(log N) time and O(N) work. ","analyze-each-step#Analyze Each Step":"Step 1 The above graph can show my program performance when setting LOOP to 100000 on different thread numbers. From the graph, we can see there are two inflection points on each subgraph. The first inflection point is on 2 threads because my work-efficient algorithm after averaging on each thread has additional parallel computing operations on my part 2 implementation (see page 1) than the sequential algorithm. The second inflection point is on 4 threads because my laptop CPU has 4 cores, and thus my program can perform best when setting to 4 threads and each core doesn’t need to switch to another thread, and this can save overhead on context switching. And with more than 4 threads, the performance of my program decreased, I think the main reason is that there is more overhead when switching threads and that might cause the performance doesn’t improve even decrease. If I setting 4 threads, each core of my 4 core CPU can get only one thread without switching to other threads. Step 2 Here, I set my LOOP to 10 and all other arguments keep the same as step1. We decrease the time of each addition operation, and thus our sequential algorithm can get the fastest result. The main reason is that our CPU can solve the problem very fast in sequential as the professor mentioned in the lecture. The decreased amount of time on each addition operation also indicates that the amount of time cost on each thread decreased, and thus more threads will result in more context switch frequency, and thus this will have more overhead than the sequential algorithm. This is the main reason when the addition operation becoming very fast, our parallel algorithm performance can’t beat the sequential algorithm. Another reason is that our parallel algorithm has more addition operation than sequential algorithm after averaging for each thread as I mentioned on page 1 of my implementation details. This also leads to a performance decrease. The above two reasons can explain why the sequential algorithm is the fastest and why the trend is like this. The above graph shows that when setting THREAD number argument to 0 and 4 (0 means sequential algorithm, 4 is because my laptop can get the best performance when threads number is 4), after setting the THREAD argument and changing the LOOP argument to test our program performance. We can see there is an inflection point on each subplot in the red circle because increasing the LOOP argument causes the amount of work on each thread to increase and thus the parallel performance can take advantage of the different workers work at the same time and that meet the performance with the sequential algorithm at the red circle on the graph. We also notice that by increasing the input size from 64 to 8k, our inflection point moves to the left as indicated in the green box on the graph. The reason is that increasing the numbers of input values, increases the addition operation to both algorithms and this has the same effect as increasing the LOOP. Thus with these two main factors, increasing the LOOP and increasing the input size, can explain why inflection points in these subgraphs of the second graph can meet earlier and earlier, and cause the trend like above. Step 3 Here, I used my own re-entrant barrier to plot the same graph as before in Step 2. My barrier is implemented with conditional variable and mutex lock. Each thread will wait and try to unlock the barrier in the queue, and once all threads get the signal, the program will go further to the next step.\nSpinlock is different from the mutex lock. As the professor said, spinlock causes the thread to keep rolling the lock to try to find out whether the stuff is unlocked or opened all the time. This will constantly cost CPU resources, and if the thread is locked for a long time, spinlock will keep trying to get the control and cause a lot more CPU resources waste.\nIn the scenarios of the small amount of work in each thread, the spinlock can be better, because spinlock will not waste much CPU time on waiting for the other threads to unlock. It might also boost some performance by trying to get control from those short lifetime threads. And thus in the contrast, in the scenario of a large amount of work in each thread, which means the mutex is better, because in this long waiting time scenarios if some thread wants to unlock and it will wait in a queue instead of waste CPU resources, and won’t constantly cost CPU time.\nIn addition, a multi-core scenario also can get benefit from the spinlock. When the critical section is small and multi-core environment can reduce the context switch time and that can take advantage of the spinlock.\nThe main drawback of the spinlock is that it will cost a lot amount of CPU time when the critical section was held for a long time in some thread, and the spinlock will keep trying to unlock the thread, which will waste the CPU resources.\nThe main drawback of the mutex is that if our program only has a small workload to deal with for each thread, then mutex is very inefficient. Because each thread will go sleep allow another thread to run, however, if the thread has a short lifespan, then the context switching will let the mutex become very inefficient.\nFrom the above two graphs compared to Step 2, we can see that my own barrier can’t get better performance than the Pthread barrier. From the first graph, we can see the inflection point meets later than the traditional Pthread barrier in Step 2. And from the second graph, we can compare to the average cost time of the program, the green box, indicating that our barrier is getting slower than the Pthread barrier but not that much, it only got 1000 ms average worse. But basically, the results are in line with my expectation, because the logic behind the Phtread is similar to mine. The main reason that my barrier is slightly inefficient is that I let my barrier spin a little bit to try to unlock the thread, which would cost some CPU time, and that will cause worse performance.\nI suggest using my own spinlock barrier to some tasks or some scenarios that have some small subtasks that only have a short lifespan, and the rest of the tasks has a large workload. In this scenario, my barrier will work better. Otherwise, if the tasks are all with a large workload, then I suggest using the Pthread barrier instead because it doesn’t waste any time on trying to unlock, it will go to sleep and wait in a queue and let other threads use the resources.\n","data#Data":" Data1: 1k Data2: 8k Data3: 16k Data4: seq_64_test ","implementation-details-for-work-efficiency-algorithm#Implementation Details for Work-Efficiency Algorithm":" I separate input data into same size blocks. (num_blocks = ceil(log2(N)))\nI compute local prefix sums for these blocks in parallel. It has O(num_blocks) = O(log N) time, and O(num_blocks) * ceil(N / num_blocks) ~ O(N) work.\nI store the last element of each blocks into an 2D array, and then I compute prefix sum of this 2D array in parallel, because this 2D array has very small size than input size, so I choose Hillis’s prefix sum algorithm to compute this 2D array in parallel. The main reason is in this step I do care about the time efficiency not the work efficiency in parallel, and this 2D array has very small size which is equal to the number of blocks, so here we should use work-inefficient algorithm which is Hillis’s prefix sum algorithm. It has O(log(N / num_blocks)) = O(log N) time, and O(N/num_blocks * log(N/num_blocks)) = O(n) work.\nAt last, add 2D array to each blocks in proper places in parallel. It also has O(num_blocks) = O(log N) time here, and also has O(num_blocks) * ceil(N / num_blocks) ~ O(N) work.\nAnalysis:\nTime: O(log N) Work: O(N) "},"title":"Exploring Multithreaded Programming with Pthreads Solving Parallel Prefix Scan Algorithm"},"/projects/supertuxkart/":{"data":{"":" Designed deep networks for a racing simulator, SuperTuxKart with Pytorch Trained linear model and multi-layer perceptron model to classify images from SuperTuxKart Trained a convolutional network to classify images from SuperTuxKart Built classification network fully convolutional and solved a semantic labeling task (labeling every pixel in the image) Implemented an object detector Trained a CNN to do vision-based self-driving in SuperTuxKart Programmed a SuperTuxKart ice-hockey player (AI that plays ice-hockey) [Resources](I also have four years of experience as a Data Scientist before pursuing a master’s degree.) "},"title":"Self-driving SuperTuxKart with Pytorch"}}